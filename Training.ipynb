{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "63m_7eC2keu7",
        "outputId": "819d7484-db80-4ba0-b162-9d4f2e9268cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t1m9592k0r2",
        "outputId": "e1263e6c-ee31-4e62-8b03-80e04bfb586b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd drive/'My Drive'/personal_projects/ta-lib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/personal_projects/ta-lib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqpFivmk6Zn",
        "outputId": "93a829ae-eb00-459d-92e2-ecab9984eb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libta-lib0.\n",
            "(Reading database ... 144618 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) ...\n",
            "Selecting previously unselected package ta-lib0-dev.\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting ta-lib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/cf/681911aa31e04ba171ab4d523a412f4a746e30d3eacb1738799d181e028b/TA-Lib-0.4.19.tar.gz (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta-lib) (1.18.5)\n",
            "Building wheels for collected packages: ta-lib\n",
            "  Building wheel for ta-lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta-lib: filename=TA_Lib-0.4.19-cp36-cp36m-linux_x86_64.whl size=1437782 sha256=7393631016c55403c931cb7fe505dd987d921b63d0e2ea0ead566493eda9fabd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/f6/12/3d1ccd06caadd8fa47e016991dd0d27f1163bb260f1854e2ff\n",
            "Successfully built ta-lib\n",
            "Installing collected packages: ta-lib\n",
            "Successfully installed ta-lib-0.4.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZx3p_Yuk-yh",
        "outputId": "9165feb3-09a5-49e5-97cb-3b94686bc69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/personal_projects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_sEsbuW7Wgb",
        "outputId": "ad4425bf-5417-4339-ce0a-732a0edecbf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Oct  5 13:12:00 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97r9YzRyf2cg"
      },
      "source": [
        "# common imports\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from talib import NATR, RSI"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17GQES1HW1uz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Masking, Concatenate, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.layers.core import Activation, Dropout, Reshape\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from hyperas import optim\n",
        "# from hyperas.distributions import choice, uniform\n",
        "# from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZrw3vRZiozd"
      },
      "source": [
        "# model_interface.py\n",
        "\n",
        "# This is an abstract class. You need to implement yours.\n",
        "class AbstractModelBuilder(object):\n",
        "\n",
        "  def __init__(self, weights_path = None):\n",
        "    self.weights_path = weights_path\n",
        "    self.model = None\n",
        "\n",
        "  def loadModel(self):\n",
        "    weights_path = self.weights_path\n",
        "    self.model = self.buildModel()\n",
        "\n",
        "    if weights_path and os.path.isfile(weights_path):\n",
        "      try:\n",
        "        self.model.load_weights(weights_path)\n",
        "      except e:\n",
        "        print(e)\n",
        "\n",
        "  # You need to override this method.\n",
        "  def buildModel(self):\n",
        "    raise NotImplementedError(\"You need to implement your own model.\")\n",
        "\n",
        "  def predict(self, data):\n",
        "    raise NotImplementedError(\"You need to implement predictor function\")\n",
        "\n",
        "  def compile_model(self):\n",
        "    raise NotImplementedError(\"You need to implement this function\")\n",
        "\n",
        "  def trainModel(self, inputs, targets):\n",
        "    raise NotImplementedError(\"You need to implement this function\")\n",
        "  \n",
        "  def printSummary(self):\n",
        "    raise NotImplementedError(\"You need to implement this function\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxfFAS0EigJi"
      },
      "source": [
        "\n",
        "# do a cut off\n",
        "# unable to build (PnL transfer t to t+1)?\n",
        "# Doesn't work\n",
        "# class lstm_model(AbstractModelBuilder):\n",
        "  \n",
        "#   def buildModel(self):\n",
        "#       inputs = []\n",
        "    \n",
        "#       weekly_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "#       inputs.append(weekly_inputs)\n",
        "    \n",
        "#       encoder_inputs = Dense(8, activation='relu')(weekly_inputs)\n",
        "#       week_encoder = LSTM(16, return_state=True)\n",
        "#       encoder_outputs, state_h, state_c = week_encoder(encoder_inputs)\n",
        "#       encoder_states = [state_h, state_c]\n",
        "\n",
        "#       input_price_info = Input(shape=(None, 3))\n",
        "#       inputs.append(input_price_info)\n",
        "#       inputs_rsi_macd_schaff = Input(shape=(None, 4))\n",
        "#       inputs.append(inputs_rsi_macd_schaff)\n",
        "#       input_bolinger = Input(shape=(None, 3))\n",
        "#       inputs.append(input_bolinger)\n",
        "#       input_adx = Input(shape(None, 3))\n",
        "#       inputs.append(input_adx)\n",
        "#       input_one_hot = Input(shape(None, 8))\n",
        "#       inputs.append(input_one_hot)\n",
        "\n",
        "#       x1 = Dense(6, activation='leaky relu')(input_price_info)\n",
        "#       x2 = Dense(6, activation='leaky relu')(input_adx)\n",
        "#       x3 = Dense(6, activation='leaky relu')(input_bolinger)\n",
        "#       x4 = Dense(8, activation='leaky relu')(input_one_hot)\n",
        "\n",
        "#       feature_vector = Concatenate(axis=1)(x1, inputs_rsi_macd_schaff, x2, x3, x4)\n",
        "#       x = Dense(64, activation='relu')(feature_vector)\n",
        "#       embedding = Dropout(0.5)(x)\n",
        "\n",
        "#       layer_1 = LSTM(64, return_sequences=True)\n",
        "#       x = layer_1(embedding, initial_state=encoder_states)\n",
        "#       output_layer_1 = Activation('leaky relu')(x)\n",
        "\n",
        "#       layer_2 = LSTM(32, return_sequences=True)\n",
        "#       x = layer_2(output_layer_1)\n",
        "#       output_layer_2 = Activation('leaky relu')(x)\n",
        "\n",
        "#       final_dense = Dense(3, activation='linear')\n",
        "#       final_output = final_dense(output_layer_2)\n",
        "\n",
        "#       model = Model(inputs, final_output)\n",
        "#       return model\n",
        "\n",
        "# Take past 30 timesteps one hot veectors then use cnn and give it to the neural net \n",
        "\n",
        "\n",
        "class cnn_model(AbstractModelBuilder):\n",
        "\n",
        "  def buildModel(self):\n",
        "    hourly_data = Input(shape=(1, 30, 3))\n",
        "\n",
        "    x = Conv2D(2, (1,3), activation='relu')(hourly_data)\n",
        "    x = MaxPooling2D(pool_size=(1,2))(x)\n",
        "    x = Conv2D(4, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2))(x)\n",
        "    x = Conv2D(4, (1,3), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x0 = Dense(8, activation='relu')(x)\n",
        "\n",
        "    input_PnL = Input(shape=(9,))\n",
        "    x1 = Dense(4)(input_PnL)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "\n",
        "    # scaling\n",
        "    input_scale = Input(shape=(30,))\n",
        "    x = Dense(4, activation='relu')(input_scale)\n",
        "    x_scale = Dense(1)(x)\n",
        "\n",
        "    inputs = [hourly_data, input_PnL, input_scale]\n",
        "    input_price_info = Input(shape=(1, 60, 2))\n",
        "    inputs.append(input_price_info)\n",
        "    input_indicators = Input(shape=(1, 60, 5))\n",
        "    inputs.append(input_indicators)\n",
        "    input_bolinger = Input(shape=(4, 60, 1))\n",
        "    inputs.append(input_bolinger)\n",
        "    input_adx = Input(shape=(1, 60, 3))\n",
        "    inputs.append(input_adx)\n",
        "    input_nb = Input(shape=(1, 30, 2))\n",
        "    inputs.append(input_nb)\n",
        "    input_onehot = Input(shape=(10, 30, 1))\n",
        "    inputs.append(input_onehot)\n",
        "\n",
        "    # price\n",
        "    x = Conv2D(5, (1,3), activation='relu')(input_price_info)\n",
        "    x = Conv2D(8, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(12, (1,3), activation='relu')(x)\n",
        "    x = Conv2D(14, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(16, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(25, activation='relu')(x)\n",
        "    x_price = Dropout(0.3)(x)\n",
        "\n",
        "    # nifty bank\n",
        "    x = Conv2D(3, (1,3), activation='relu')(input_nb)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(4, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(5, (1,3), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x_nb = Dense(6)(x)\n",
        "\n",
        "    # adx\n",
        "    x = Dense(1)(input_adx)\n",
        "    x = LeakyReLU()(x)\n",
        "    indicators = Concatenate(axis=3)([input_indicators, x, input_adx[:,:,:,1:]])\n",
        "\n",
        "    # bolinger\n",
        "    x = Conv2D(3, (4,3), activation='relu')(input_bolinger)\n",
        "    x = Conv2D(4, (1,5), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(5, (1,7), activation='relu')(x)\n",
        "    x = Conv2D(6, (1,10), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(10, activation='relu')(x)\n",
        "    x_bolinger = Dropout(0.3)(x)\n",
        "\n",
        "    # onehot\n",
        "    x = Conv2D(8, (8,3), activation='relu')(input_onehot)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(10, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(12, (1,5), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(10, 'relu')(x)\n",
        "    x_onehot = Dropout(0.2)(x)\n",
        "\n",
        "    #indicators\n",
        "    x = Conv2D(8, (1,3), activation='relu')(indicators)\n",
        "    x = Conv2D(12, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x) \n",
        "    x = Conv2D(12, (1,3), activation='relu')(x)\n",
        "    x = Conv2D(16, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(20, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(20, activation='relu')(x)\n",
        "    x_indicators = Dropout(0.3)(x)\n",
        "\n",
        "    # concatenate and combine all the feature_vectors\n",
        "    x = Concatenate(axis=1)([x_price, x_bolinger, x_indicators, x_onehot, x_nb])\n",
        "    x = Dense(30)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # concatenate context\n",
        "    x = Concatenate(axis=1)([x, x1, x0])\n",
        "    x = Dense(23)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Dense(12)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    # Experiment with tanh too\n",
        "    x = Dense(3, activation='linear')(x)\n",
        "\n",
        "    # provide scaling\n",
        "    # possible to force x_scale > 0 ? \n",
        "    final_output = x * x_scale\n",
        "\n",
        "    # num trainable params large ~ 20k ? \n",
        "    model = Model(inputs, final_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  def compile_model(self):\n",
        "    self.model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "  def predict(self, data, sub_sample=None):\n",
        "    if sub_sample is None:\n",
        "      return self.model.predict([data['hourly_data'],   \n",
        "                      data['portfolio'],\n",
        "                      data['scale'], \n",
        "                      data['price_curve'],\n",
        "                      data['indicators'],\n",
        "                      data['bolinger'],\n",
        "                      data['adx'],\n",
        "                      data['nb'],\n",
        "                      data['onehot']])\n",
        "    else:\n",
        "      return self.model.predict([data['hourly_data'][sub_sample],   \n",
        "                      data['portfolio'][sub_sample],\n",
        "                      data['scale'][sub_sample], \n",
        "                      data['price_curve'][sub_sample],\n",
        "                      data['indicators'][sub_sample],\n",
        "                      data['bolinger'][sub_sample],\n",
        "                      data['adx'][sub_sample],\n",
        "                      data['nb'][sub_sample],\n",
        "                      data['onehot'][sub_sample]])\n",
        "\n",
        "\n",
        "  def batchTrain(self, data, targets):\n",
        "    return self.model.train_on_batch([data['hourly_data'],    \n",
        "                      data['portfolio'],\n",
        "                      data['scale'], \n",
        "                      data['price_curve'],\n",
        "                      data['indicators'],\n",
        "                      data['bolinger'],\n",
        "                      data['adx'],\n",
        "                      data['nb'],\n",
        "                      data['onehot']],\n",
        "                      targets)\n",
        "  \n",
        "  def fitModel(self, data, targets, batch_size, epoch):\n",
        "    return self.model.fit([data['hourly_data'],    \n",
        "                      data['portfolio'],\n",
        "                      data['scale'], \n",
        "                      data['price_curve'],\n",
        "                      data['indicators'],\n",
        "                      data['bolinger'],\n",
        "                      data['adx'],\n",
        "                      data['nb'],\n",
        "                      data['onehot']],\n",
        "                      targets,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epoch)\n",
        "  \n",
        "  def printSummary(self):\n",
        "    print(self.model.summary())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxPFqMFmP9yO"
      },
      "source": [
        "# Improvements\n",
        "# prioritized Experience Replay\n",
        "# Dual Target Network\n",
        "# Better Optimzation techique\n",
        "class ExperienceReplay(object):\n",
        "  \n",
        "  def __init__(self, max_memory=200000, discount=.9):\n",
        "    self.max_memory = max_memory\n",
        "    self.count_range = {}\n",
        "    self.memory = list()\n",
        "    self.discount = discount\n",
        "    self.record_sort = False\n",
        "\n",
        "  def remember(self, states, game_over):\n",
        "    for i in range(p_games):\n",
        "      save_states_tm1, save_states_t = {}, {}\n",
        "      for key in states[0].keys():\n",
        "        save_states_tm1[key] = states[0][key][i]\n",
        "        save_states_t[key] = states[3][key][i]\n",
        "      self.memory.append([[save_states_tm1, int(states[1][i]), states[2][i][0], save_states_t], game_over, 0])\n",
        "    if self.record_sort:\n",
        "      if self.count_range.__contains__(0):\n",
        "        l, r = self.count_range[0]\n",
        "        self.count_range[0] = (l, r + len(states[0]))\n",
        "      else:\n",
        "        self.count_range[0] = (len(self.memory) - 1, len(self.memory) - 1 + len(states[0]))\n",
        "    \n",
        "    if len(self.memory) > self.max_memory:\n",
        "      del self.memory[:30000]\n",
        "\n",
        "  # keep record sorted\n",
        "  def _adjust_record(self, idx):\n",
        "    if not self.record_sort:\n",
        "      return\n",
        "\n",
        "    old_count = self.memory[idx][2]\n",
        "    self.memory[idx][2] = old_count + 1\n",
        "    old_l,old_r = self.count_range[old_count]\n",
        "  \n",
        "    if idx != old_l:\n",
        "      temp = self.memory[idx]\n",
        "      self.memory[idx] = self.memory[old_l]\n",
        "      self.memory[old_l] = temp\n",
        "\n",
        "    if self.count_range.__contains__(old_count + 1):\n",
        "      new_l,new_r = self.count_range[old_count + 1]\n",
        "      self.count_range[old_count + 1] = (new_l, new_r + 1)\n",
        "    else:\n",
        "      self.count_range[old_count + 1] = (old_l, old_l)\n",
        "\n",
        "    if old_l + 1 > old_r:\n",
        "      del self.count_range[old_count - 1]\n",
        "    else:\n",
        "      self.count_range[old_count - 1] = (old_l + 1, old_r)\n",
        "\n",
        "  # use it for fast speed (uses a lot more memory)\n",
        "  def get_offline_batch(self, env, model_interface, batch_size):\n",
        "    len_memory = len(self.memory)\n",
        "    size = min(len_memory, batch_size)\n",
        "    num_actions = model_interface.model.output_shape[-1]\n",
        "    inputs = {}\n",
        "    inputs_t = {}\n",
        "    reward = np.empty(size)\n",
        "    over = np.empty(size)\n",
        "    action = np.zeros((size, num_actions), dtype=bool)\n",
        "    price_curve, indicators, bolinger, adx, nb, onehot, portfolio, hourly_data, scale_data = self.allocate(size)\n",
        "    price_curve_t, indicators_t, bolinger_t, adx_t, nb_t, onehot_t, portfolio_t, hourly_data_t, scale_data_t = self.allocate(size)\n",
        "\n",
        "    for i, idx in enumerate(np.random.randint(0, len_memory, size=size)):\n",
        "      state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
        "      \n",
        "      price_curve[i] = state_t['price_curve']\n",
        "      indicators[i] = state_t['indicators']\n",
        "      bolinger[i] = state_t['bolinger']\n",
        "      adx[i] = state_t['adx']\n",
        "      onehot[i] = state_t['onehot']\n",
        "      nb[i] = state_t['nb']\n",
        "      portfolio[i] = state_t['portfolio']\n",
        "      hourly_data[i] = state_t['hourly_data']\n",
        "      scale_data[i] = state_t['scale']\n",
        "      \n",
        "      price_curve_t[i] = state_tp1['price_curve']\n",
        "      indicators_t[i] = state_tp1['indicators']\n",
        "      bolinger_t[i] = state_tp1['bolinger']\n",
        "      adx_t[i] = state_tp1['adx']\n",
        "      onehot_t[i] = state_tp1['onehot']\n",
        "      nb_t[i] = state_tp1['nb']\n",
        "      portfolio_t[i] = state_tp1['portfolio']\n",
        "      hourly_data_t[i] = state_tp1['hourly_data']\n",
        "      scale_data_t[i] = state_tp1['scale']\n",
        "\n",
        "      reward[i] = reward_t\n",
        "      action[i][action_t] = True\n",
        "      over[i] = float(self.memory[idx][1])\n",
        "    \n",
        "    inputs['price_curve'] = price_curve\n",
        "    inputs['indicators'] = indicators\n",
        "    inputs['bolinger'] = bolinger\n",
        "    inputs['adx'] = adx\n",
        "    inputs['onehot'] = onehot\n",
        "    inputs['portfolio'] = portfolio\n",
        "    inputs['nb'] = nb\n",
        "    inputs['scale'] = scale_data\n",
        "    inputs['hourly_data'] = hourly_data\n",
        "\n",
        "    inputs_t['price_curve'] = price_curve_t\n",
        "    inputs_t['indicators'] = indicators_t\n",
        "    inputs_t['bolinger'] = bolinger_t\n",
        "    inputs_t['adx'] = adx_t\n",
        "    inputs_t['onehot'] = onehot_t\n",
        "    inputs_t['portfolio'] = portfolio_t\n",
        "    inputs_t['nb'] = nb_t\n",
        "    inputs_t['scale'] = scale_data_t\n",
        "    inputs_t['hourly_data'] = hourly_data_t\n",
        "\n",
        "    targets = model_interface.predict(inputs)\n",
        "    targets[action] = reward + self.discount * over * np.max(model_interface.predict(inputs_t), axis=1)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "  def allocate(self, size):\n",
        "    p_s = list(self.memory[0][0][0]['price_curve'].shape)\n",
        "    i_s = list(self.memory[0][0][0]['indicators'].shape)\n",
        "    b_s = list(self.memory[0][0][0]['bolinger'].shape)\n",
        "    a_s = list(self.memory[0][0][0]['adx'].shape)\n",
        "    o_s = list(self.memory[0][0][0]['onehot'].shape)\n",
        "    pf_s = list(self.memory[0][0][0]['portfolio'].shape)\n",
        "    n_s = list(self.memory[0][0][0]['nb'].shape)\n",
        "    s_s = list(self.memory[0][0][0]['scale'].shape)\n",
        "    h_s = list(self.memory[0][0][0]['hourly_data'].shape) \n",
        "\n",
        "    return np.empty([size] + p_s), np.empty([size] + i_s), np.empty([size] + b_s), np.empty([size] + a_s), np.empty([size] + n_s), np.empty([size] + o_s), np.empty([size] + pf_s), np.empty([size] + h_s), np.empty([size] + s_s)\n",
        "\n",
        "  # used for low memory usage\n",
        "  def get_online_batch(self, env, model_interface, batch_size=64):\n",
        "    len_memory = len(self.memory)\n",
        "    num_actions = model_interface.model.output_shape[-1]\n",
        "    inputs = {}\n",
        "    price_curve, indicators, bolinger, adx, nb, onehot, portfolio, hourly_data, scale_data = [], [], [], [], [], [], [], [], []\n",
        "\n",
        "    targets = np.zeros((min(len_memory, batch_size), num_actions))\n",
        "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):\n",
        "      pnl_state_t, action_t, reward_t, pnl_state_tp1, code, day, index = self.memory[idx][0]\n",
        "      self._adjust_record(idx)\n",
        "      state_t = env.generate_data(code, day, index)\n",
        "      state_t['portfolio'] = pnl_state_t\n",
        "      game_over = self.memory[idx][1]\n",
        "      targets[i] = model_interface.predict(state_t)[0]\n",
        "      if not game_over:\n",
        "        state_tp1 = env.generate_data(code, day, index + 1)\n",
        "        state_tp1['portfolio'] = pnl_state_tp1\n",
        "        Q_sa = np.max(model_interface.predict(state_tp1)[0])\n",
        "        targets[i, action_t] = reward_t + self.discount * Q_sa\n",
        "      else:\n",
        "        targets[i, action_t] = reward_t\n",
        "\n",
        "      price_curve.append(np.squeeze(state_t['price_curve'], axis=0))\n",
        "      indicators.append(np.squeeze(state_t['indicators'], axis=0))\n",
        "      bolinger.append(np.squeeze(state_t['bolinger'], axis=0))\n",
        "      adx.append(np.squeeze(state_t['adx'], axis=0))\n",
        "      onehot.append(np.squeeze(state_t['onehot'], axis=0))\n",
        "      nb.append(np.squeeze(state_t['nb'], axis=0))\n",
        "      portfolio.append(np.squeeze(state_t['portfolio'], axis=0))\n",
        "      hourly_data.append(np.squeeze(state_t['hourly_data'], axis=0))\n",
        "      scale_data.append(np.squeeze(state_t['scale'], axis=0))\n",
        "\n",
        "\n",
        "    inputs['price_curve'] = np.array(price_curve)\n",
        "    inputs['indicators'] = np.array(indicators)\n",
        "    inputs['bolinger'] = np.array(bolinger)\n",
        "    inputs['adx'] = np.array(adx)\n",
        "    inputs['onehot'] = np.array(onehot)\n",
        "    inputs['portfolio'] = np.array(portfolio)\n",
        "    inputs['nb'] = np.array(nb)\n",
        "    inputs['scale'] = np.array(scale_data)\n",
        "    inputs['hourly_data'] = np.array(hourly_data)\n",
        "\n",
        "    return inputs, targets"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YbXipbUeJD"
      },
      "source": [
        "path_list = ['BAJFINANCE/processed_data', 'AXISBANK/processed_data', 'HDFC/processed_data', 'HDFCBANK/processed_data', 'ICICIBANK/processed_data']\n",
        "X_price_info, X_indicators, X_adx, X_bolinger, X_onehot, X_nb = [{} for _ in range(6)]\n",
        "X_avg_val, X_high_val, X_low_val, X_close_val = [{} for _ in range(4)]\n",
        "\n",
        "code = 0\n",
        "for path in path_list:\n",
        "  if not os.path.exists(path):\n",
        "    print('path not found', path)\n",
        "  else:\n",
        "    f = open(path, 'rb')\n",
        "    obj = pickle.load(f)\n",
        "    X_price_info[code] = obj['X_price_info']\n",
        "    X_indicators[code] = obj['X_indicators']\n",
        "    X_adx[code] = obj['X_adx']\n",
        "    X_bolinger[code] = obj['X_bolinger']\n",
        "    X_onehot[code] = obj['X_onehot']\n",
        "    X_nb[code] = obj['X_nb']\n",
        "    X_avg_val[code] = obj['avg_val']\n",
        "    X_high_val[code] = obj['high_val']\n",
        "    X_low_val[code] = obj['low_val']\n",
        "    X_close_val[code] = obj['close_val']\n",
        "    code += 1\n",
        "    f.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_Q6rVQVwys"
      },
      "source": [
        "# from keras.optimizers import SGD\n",
        "modelFilename=''\n",
        "model_interface = cnn_model(modelFilename)\n",
        "model_interface.loadModel()\n",
        "model_interface.compile_model()\n",
        "\n",
        "# sgd = SGD(lr = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "# need a better optimization/random optimization techique.\n",
        "# maybe try Particle swarm or genectic algorithm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YLQcjc4XN4Q"
      },
      "source": [
        "# Initialize experience replay object\n",
        "exp_replay = ExperienceReplay()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oknUVPvpYE0G"
      },
      "source": [
        "temp = exp_replay.memory"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3ixD4uWYIaV"
      },
      "source": [
        "exp_replay.memory = temp"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPEdWI3BYPXg",
        "outputId": "03622edd-f9c8-40b7-edb7-0f6e1322a403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(exp_replay.memory)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA0yR1fUadZd",
        "outputId": "7520f8e6-0890-4bd5-8ff0-665847885ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.argmax([1,2,4])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1cdge8Us6W",
        "outputId": "e8686407-63bb-4e17-b837-229f99ebc56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_interface.predict(input_t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.4604977e-04,  2.3279474e-05, -7.8652592e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJVFDDRiOJlZ"
      },
      "source": [
        "model_interface.model.save_weights(\"model_checkpoint_day_155.h5\" if modelFilename == None else modelFilename, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nna8LW2FS05b"
      },
      "source": [
        "scope = 60\n",
        "p_games = 100\n",
        "env = MarketEnv(X_price_info,\n",
        "        X_indicators,\n",
        "        X_bolinger,\n",
        "        X_adx,\n",
        "        X_onehot,\n",
        "        X_nb,\n",
        "        X_avg_val,\n",
        "        X_close_val,\n",
        "        X_high_val,\n",
        "        X_low_val,\n",
        "        scope,\n",
        "        parallel_games=p_games)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DdTcW0OsrxZ",
        "outputId": "1c2b7d1b-7c52-4083-e96a-5c38012e2dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# parameters\n",
        "epsilon = 0.5  # exploration\n",
        "large_batches = True  # train in large batches for less I/O overhead.\n",
        "min_epsilon = 0.01\n",
        "batch_size = 16384 if large_batches else 64\n",
        "discount = 0.9    # increasing future discount incentivise future P\n",
        "epoch = 3\n",
        "loss = 0.\n",
        "stop = False\n",
        "save_cycle = 3 # epoch // 25\n",
        "action = np.zeros(p_games)\n",
        "\n",
        "for e in range(epoch):\n",
        "  input_t = env.reset()\n",
        "  day_end = False\n",
        "  cumReward, realized_pnl, per_success, num_trades = 0., 0., 0, 0\n",
        "  \n",
        "  while not day_end:\n",
        "    input_tm1 = input_t\n",
        "    isRandom = np.random.rand(p_games) <= epsilon\n",
        "    action[isRandom] = np.random.randint(0, 3, size=np.sum(isRandom))\n",
        "    if len(isRandom) < p_games:\n",
        "      prediction = model_interface.predict(input_tm1, np.logical_not(isRandom))\n",
        "      if np.nan in prediction:\n",
        "        print(\"OCCUR NaN!!!\")\n",
        "        stop = True\n",
        "        break\n",
        "      action[np.logical_not(isRandom)] = np.argmax(prediction, axis=1)\n",
        "    input_t, reward, pnl, success, day_end = env.step(action)\n",
        "    exp_replay.remember([input_tm1, action, reward, input_t], day_end)\n",
        "    cumReward += np.sum(reward)\n",
        "    realized_pnl += np.sum(pnl)\n",
        "    per_success += np.sum(success[success > 0])\n",
        "    num_trades += np.sum(np.abs(success))\n",
        "\n",
        "  \n",
        "  print('epoch =', str(e), 'cumReward =', str(cumReward / p_games), 'pnl =', str(realized_pnl / p_games), 'percentage success', str(float(per_success) * 100 / max(num_trades, 1)))\n",
        "  \n",
        "  if stop:\n",
        "    break\n",
        "  \n",
        "  # train\n",
        "  if large_batches:\n",
        "    inputs, targets = exp_replay.get_offline_batch(env, model_interface, batch_size)\n",
        "    model_interface.fitModel(inputs, targets, 64, 10)\n",
        "  else:\n",
        "    inputs, targets = exp_replay.get_online_batch(env, model_interface)\n",
        "    loss = model_interface.batchTrain(inputs, targets)\n",
        "    print('batch = ', str(e), 'loss = ', str(loss))\n",
        "  \n",
        "  if not (e + 1) % save_cycle:\n",
        "    epsilon = max(min_epsilon, epsilon * 0.95)\n",
        "    model_interface.model.save_weights(\"model.h5\", overwrite=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0 cumReward = 0.052081829135809145 pnl = -0.007215079192740983 percentage success 48.71656309105647\n",
            "Epoch 1/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.8200e-04\n",
            "Epoch 2/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.0706e-04\n",
            "Epoch 3/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 9.4139e-05\n",
            "Epoch 4/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.9351e-05\n",
            "Epoch 5/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.8702e-05\n",
            "Epoch 6/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.4195e-05\n",
            "Epoch 7/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.2106e-05\n",
            "Epoch 8/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.1229e-05\n",
            "Epoch 9/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.1335e-05\n",
            "Epoch 10/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 8.1218e-05\n",
            "epoch = 1 cumReward = 0.06366773873076789 pnl = 0.0009021864665444379 percentage success 51.23192287091591\n",
            "Epoch 1/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 3.2554e-05\n",
            "Epoch 2/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 3.1234e-05\n",
            "Epoch 3/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 3.0695e-05\n",
            "Epoch 4/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 2.9724e-05\n",
            "Epoch 5/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 3.0680e-05\n",
            "Epoch 6/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 2.9718e-05\n",
            "Epoch 7/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 2.9007e-05\n",
            "Epoch 8/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 2.9563e-05\n",
            "Epoch 9/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 2.9039e-05\n",
            "Epoch 10/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 2.9965e-05\n",
            "epoch = 2 cumReward = 0.05400818734105567 pnl = -0.0009451331875511549 percentage success 48.65229110512129\n",
            "Epoch 1/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.6759e-05\n",
            "Epoch 2/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.6241e-05\n",
            "Epoch 3/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.4988e-05\n",
            "Epoch 4/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.5018e-05\n",
            "Epoch 5/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.5355e-05\n",
            "Epoch 6/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.5072e-05\n",
            "Epoch 7/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.5612e-05\n",
            "Epoch 8/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.5077e-05\n",
            "Epoch 9/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.4901e-05\n",
            "Epoch 10/10\n",
            "256/256 [==============================] - 2s 9ms/step - loss: 1.5077e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSm29KNoVN26",
        "outputId": "a318d7dd-2398-49ba-ee80-301391a98720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env.position"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 0.],\n",
              "       [ 0.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [ 0.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 0.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 0.],\n",
              "       [ 0.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [-1.],\n",
              "       [ 1.],\n",
              "       [ 1.],\n",
              "       [-1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZZSxmlKb9Kn"
      },
      "source": [
        "class MarketEnv(object):\n",
        "\n",
        "    ON_PROFIT_HOLDING = 0.9\n",
        "\n",
        "    def __init__(self, x_price_info, x_indicators, x_bolinger, x_adx, x_onehot, x_nb, x_avg_val, x_close_val, x_high_val, x_low_val, scope, parallel_games=1):\n",
        "        \n",
        "        max_size = 0\n",
        "        self.index_end = []\n",
        "        self.num_target = X_price_info.__len__()\n",
        "        for key in X_price_info.keys():\n",
        "            self.index_end.append(len(X_price_info[key]))\n",
        "            max_size = max(max_size, self.index_end[-1])\n",
        "        \n",
        "        self.X_price_info = self._copy_table(x_price_info, max_size)\n",
        "        self.X_indicators = self._copy_table(x_indicators, max_size)\n",
        "        self.X_bolinger = self._copy_table(x_bolinger, max_size)\n",
        "        self.X_adx = self._copy_table(x_adx, max_size)\n",
        "        self.X_onehot = self._copy_table(x_onehot, max_size)\n",
        "        self.X_nb = self._copy_table(x_nb, max_size)\n",
        "        \n",
        "        self.X_avg_val = x_avg_val\n",
        "        self.X_close_val = x_close_val\n",
        "        self.X_high_val = x_high_val\n",
        "        self.X_low_val = x_low_val\n",
        "        self.scope = scope\n",
        "        self.parallel_games = parallel_games\n",
        "        self.done = False\n",
        "        self.position, self.reward, self.boughts, self.budget, self.position, self.realized_pnl, self.success, self.num_position, self.cum_reward = [np.zeros((parallel_games, 1)) for _ in range(9)]\n",
        "        self.num_days = self.X_price_info.shape[1]\n",
        "        self.episode_length = self.X_price_info.shape[2]\n",
        "        self.targetContext = np.zeros((self.num_target, self.num_days), dtype=bool)\n",
        "\n",
        "        self.actions = [\n",
        "            \"LONG\",\n",
        "            \"HOLD\",\n",
        "            \"SHORT\"\n",
        "        ]\n",
        "\n",
        "        self.ON_LOSS_HOLDING = np.array([1.5, 1.2, 1.0, 0.8, 0.3, 0.9, 2.0, 3.0, 4.0])\n",
        "\n",
        "        # self.action_space = spaces.Discrete(len(self.actions))\n",
        "        # self.observation_space = spaces.Box(np.ones(scope * (len(input_codes) + 1)) * -1, np.ones(scope * (len(input_codes) + 1)))\n",
        "\n",
        "        # self.reset()\n",
        "        self._seed()\n",
        "    \n",
        "    def _copy_table(self, x_array, max_size):\n",
        "        X_array = np.empty([self.num_target, max_size] + list(x_array[0].shape[1:]))\n",
        "        for i in range(self.num_target):\n",
        "            diff = max_size - x_array[i].shape[0]\n",
        "            if diff > 0:\n",
        "                X_array[i] = np.concatenate([x_array[i], np.zeros([diff] + list(x_array[0].shape[1:]))], axis=0)\n",
        "            else:\n",
        "                X_array[i] = x_array[i]\n",
        "        return X_array\n",
        "\n",
        "    def _get_holding_mutiplier(self, cum_reward, diff):     \n",
        "        return (diff < 0) * self.ON_LOSS_HOLDING[np.clip(((2.0 - cum_reward) / 0.5).astype(int), 0, 8)] + (diff >= 0) * self.ON_PROFIT_HOLDING\n",
        "\n",
        "    def _create_hourly_price_till(self, index, timesteps, code):\n",
        "        minutes = timesteps * 60\n",
        "        cur = np.zeros(self.parallel_games, dtype=int)\n",
        "        hourly_avg_price, hourly_close_price, hourly_high_price, hourly_low_price = [np.zeros((self.parallel_games, timesteps)) for _ in range(4)]\n",
        "\n",
        "        for k in range(self.parallel_games):\n",
        "            start = max((index[k] + 1) % 60, index[k] - minutes + 1)\n",
        "            end = min(len(self.X_close_val[code[k]]), index[k] + 1)\n",
        "            for i in np.arange(start, end, 60):\n",
        "                hourly_avg_price[k][cur[k]] = np.average(self.X_avg_val[code[k]][i: i + 60])\n",
        "                hourly_close_price[k][cur[k]] = self.X_close_val[code[k]][i + 59]\n",
        "                hourly_high_price[k][cur[k]] = np.max(self.X_high_val[code[k]][i: i + 60])\n",
        "                hourly_low_price[k][cur[k]] = np.min(self.X_low_val[code[k]][i: i + 60])\n",
        "                cur[k] += 1\n",
        "        \n",
        "        return (hourly_avg_price, hourly_close_price, hourly_high_price, hourly_low_price, cur)\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.state, self.reward, self.realized_pnl, self.success, self.done\n",
        "\n",
        "        vari = np.expand_dims(self.target_price[:, self.currentTargetIndex, 1] / 100, axis=1)\n",
        "        self.budget = self.budget / (1 + vari)\n",
        "        change = self.boughts * vari\n",
        "        self.boughts += change\n",
        "        self.cum_reward = self.boughts - self.num_position * self.position\n",
        "        action = np.expand_dims(1 - action, axis=1)\n",
        "        distribute = np.abs(np.sign(action + self.position))\n",
        "        self.realized_pnl = (1 - distribute) * self.cum_reward\n",
        "        self.reward = self.realized_pnl + distribute * self._get_holding_mutiplier(self.cum_reward, change) * change\n",
        "        new_position = (self.budget >= 1) * action * distribute\n",
        "        self.budget += (1 - distribute) * (self.num_position + self.cum_reward) - np.abs(new_position)\n",
        "        self.position = np.sign(self.position * distribute + new_position)\n",
        "        self.boughts = self.boughts * distribute + new_position\n",
        "        self.num_position = np.abs(self.position * self.num_position + new_position)\n",
        "        self.success = np.sign(self.realized_pnl)\n",
        "        self.currentTargetIndex += 1\n",
        "\n",
        "        # print('budget=',str(self.budget[0][0]),'bought=',str(self.boughts[0][0]),'position=',str(self.position[0][0]),'distribute=',str(distribute[0][0]),'new_position=',str(new_position[0][0]),'num_position=',str(self.num_position[0][0]),'cum_reward=',str(self.cum_reward[0][0]))\n",
        "\n",
        "        if self.currentTargetIndex >= self.episode_length:\n",
        "            self.done = True\n",
        "        \n",
        "        self._defineState()\n",
        "        return self.state, self.reward, self.realized_pnl, self.success, self.done\n",
        "\n",
        "    def reset(self):\n",
        "        self.targetContext *= False\n",
        "        x = np.array([np.random.rand() for _ in range(self.num_target)])\n",
        "        x = (x / np.sum(x)) * self.parallel_games\n",
        "        y = []\n",
        "        for i in range(self.num_target - 1):\n",
        "            y.append(int(x[i]))\n",
        "        y.append(self.parallel_games - sum(y))\n",
        "        y = np.random.permutation(y)\n",
        "        self.targetDays = []\n",
        "        self.targetCode = np.hstack([i * np.ones(y[i], dtype=int) for i in range(self.num_target)])\n",
        "        for i in range(self.num_target):\n",
        "            selection = np.sort(np.random.choice(range(8, self.index_end[i]), size=y[i], replace=False))\n",
        "            self.targetContext[i][selection] = True\n",
        "            self.targetDays = self.targetDays + list(selection)\n",
        "        \n",
        "        self.targetDays = np.array(self.targetDays)\n",
        "        self.target_price = self.X_price_info[self.targetContext]\n",
        "        self.target_indicators = self.X_indicators[self.targetContext]\n",
        "        self.target_nb = self.X_nb[self.targetContext]\n",
        "        self.target_bolinger = self.X_bolinger[self.targetContext]\n",
        "        self.target_adx = self.X_adx[self.targetContext]\n",
        "        self.target_onehot = self.X_onehot[self.targetContext]\n",
        "\n",
        "        self.currentTargetIndex = self.scope - 1\n",
        "        self.done = False\n",
        "        self.budget = self.budget * 0. + 8. \n",
        "        self.position = self.position * 0.\n",
        "        self.num_position = self.num_position * 0.\n",
        "        self.boughts = self.boughts * 0.\n",
        "        self.cum_reward = self.cum_reward * 0.\n",
        "        self._defineState()\n",
        "        return self.state\n",
        "\n",
        "    def _render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "        return self.state\n",
        "\n",
        "    def _seed(self):\n",
        "        return int(random.random() * 100)\n",
        "\n",
        "    def _create_hourly_data(self, code, day, index):\n",
        "        timesteps = 80\n",
        "        lookback = self.scope // 2\n",
        "        # adjusting day_ignored and index_ignored\n",
        "        end = (day + 4) * 375 + (index + 30)\n",
        "        hourly_avg_val, hourly_close_val, hourly_high_val, hourly_low_val, j = self._create_hourly_price_till(end, timesteps, code)\n",
        "        hourly_rsi, hourly_atr = [], []\n",
        "        X_avg_price = np.empty((self.parallel_games, 1, lookback, 1))\n",
        "        for i in range(self.parallel_games):\n",
        "          hourly_rsi.append((RSI(hourly_close_val[i][:j[i]], timeperiod=14)[-lookback:]) / 100)\n",
        "          hourly_atr.append(NATR(hourly_high_val[i][:j[i]], hourly_low_val[i][:j[i]], hourly_close_val[i][:j[i]], timeperiod=14)[-lookback:])\n",
        "          X_avg_price[i] = np.expand_dims((hourly_avg_val[i][j[i]-lookback:j[i]] - hourly_avg_val[i][j[i]-(lookback + 1) : j[i]-1]) * 10 / hourly_avg_val[i][j[i]-(lookback + 1) : j[i]-1], axis=(0, 2))\n",
        "        hourly_rsi = np.expand_dims(np.array(hourly_rsi), axis=(1, 3))\n",
        "        hourly_atr = np.expand_dims(np.array(hourly_atr), axis=(1, 3))\n",
        "        hourly_data = np.concatenate([X_avg_price, hourly_rsi, hourly_atr], axis=3)\n",
        "\n",
        "        return hourly_data\n",
        "\n",
        "    def _standardize(self, X_array):\n",
        "        n = X_array.shape[2]\n",
        "        for i in range(n):\n",
        "            temp = X_array[:, :, i]\n",
        "            X_array[:, :, i] = (temp - np.mean(temp, axis=1, keepdims=True)) / np.std(temp, axis=1, keepdims=True)\n",
        "\n",
        "    # to prevent memory overflow create training data in an online way\n",
        "    def generate_data(self, code, day, index):\n",
        "        r_data = {}\n",
        "\n",
        "        if index < self.scope - 1:\n",
        "            print('index not in range =', str(index))\n",
        "            return None\n",
        "\n",
        "        # normalize prices\n",
        "        price_curve = np.array(self.target_price[:, index - self.scope + 1 : index + 1])\n",
        "        r_data['scale'] = np.array(price_curve[:, self.scope // 2:, 1])\n",
        "        self._standardize(price_curve)\n",
        "        price_curve = np.expand_dims(price_curve, axis=1)\n",
        "\n",
        "        # normalize nb prices\n",
        "        nb_price = np.array(self.target_nb[:, index - (self.scope // 2) + 1 : index + 1])\n",
        "        self._standardize(nb_price)\n",
        "        nb_price = np.expand_dims(nb_price, axis=1)\n",
        "\n",
        "        indicators = self.target_indicators[:, index - self.scope + 1 : index + 1]\n",
        "        indicators = np.expand_dims(indicators, axis=1)\n",
        "        \n",
        "        bolinger = self.target_bolinger[:, index - self.scope + 1 : index + 1]\n",
        "        bolinger = np.swapaxes(bolinger, 1, 2)\n",
        "        bolinger = np.expand_dims(bolinger, axis=3)\n",
        "        \n",
        "        adx = self.target_adx[:, index - self.scope + 1 : index + 1]\n",
        "        adx = np.expand_dims(adx, axis=1)\n",
        "        \n",
        "        onehot = self.target_onehot[:, index - (self.scope // 2) + 1 : index + 1]\n",
        "        onehot = np.swapaxes(onehot, 1, 2)\n",
        "        onehot = np.expand_dims(onehot, axis=3)\n",
        "\n",
        "        r_data['price_curve'] = price_curve\n",
        "        r_data['indicators'] = indicators\n",
        "        r_data['bolinger'] = bolinger\n",
        "        r_data['adx'] = adx\n",
        "        r_data['onehot'] = onehot\n",
        "        r_data['nb'] = nb_price\n",
        "        r_data['hourly_data'] = self._create_hourly_data(code, day, index)\n",
        "\n",
        "        return r_data\n",
        "\n",
        "    def _defineState(self):\n",
        "        if self.done:\n",
        "            return\n",
        "        \n",
        "        tmpState = self.generate_data(self.targetCode, self.targetDays, self.currentTargetIndex)\n",
        "        \n",
        "        onehot_code = np.zeros((self.parallel_games, self.num_target), dtype=float)\n",
        "        for i in range(self.parallel_games):\n",
        "            onehot_code[self.targetCode[i]] = 1.0\n",
        "        \n",
        "        tmpState['portfolio'] = np.concatenate([self.cum_reward / 8, self.budget / 8, self.position, (float(self.currentTargetIndex) / self.episode_length) * np.ones((self.parallel_games, 1)), onehot_code], axis=1)\n",
        "        self.state = tmpState\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import gym\n",
        "# from gym import spaces\n",
        "\n",
        "# idea don't penalize strictly on holding even when the price falls upto a certain level\n",
        "# use the idea of stoploss i.e when the price fall below stoploss start penalizing heavily for subsequent drops\n",
        "# create levels:\n",
        "\n",
        "# rise :\n",
        "# All values  :  MULTIPLIER = 0.9\n",
        "\n",
        "# fall :\n",
        "# [   > 1.5 )%  :  MULTIPLIER = 1.5\n",
        "# [1.5 - 0.5)%  :  MULTIPLIER = 1.0\n",
        "# [0.5 - 0)%    :  MULTIPLIER = 0.8\n",
        "# [0 -  0.5)%   :  MULTIPLIER = 0.2\n",
        "# [0.5  - 1)%   :  MULTIPLIER = 1.0\n",
        "# [1 -  1.5)%   :  MULTIPLIER = 2.0\n",
        "# [1.5 -  2)%   :  MULTIPLIER = 3.0\n",
        "# [ >=   2 )%   :  MULTIPLIER = 4.0\n",
        "# [ may force it to sell ] ?\n",
        "        # if self.actions[action] == \"LONG\":\n",
        "        #     if self.position == -1:\n",
        "        #         for b in self.boughts:\n",
        "        #             self.budget += -b\n",
        "        #             self.reward += -(b + 1)\n",
        "                \n",
        "        #         realized_pnl = self.reward\n",
        "        #         if self.cumulative_reward:\n",
        "        #             self.reward = self.reward / max(1, len(self.boughts))\n",
        "                \n",
        "        #         # if got bankrupt\n",
        "        #         if self.sudden_death * len(self.boughts) > self.reward:\n",
        "        #             self.nextDay = True\n",
        "\n",
        "        #         self.boughts = []\n",
        "        #         self.position = 0\n",
        "        #     elif self.budget > 0:\n",
        "        #         self.boughts.append(1.0)\n",
        "        #         self.position = 1\n",
        "        #         self.budget -= 1.\n",
        "        \n",
        "        # elif self.actions[action] == \"SHORT\":\n",
        "        #     if self.position == 1:\n",
        "        #         for b in self.boughts:\n",
        "        #             self.budget += b\n",
        "        #             self.reward += b - 1\n",
        "                \n",
        "        #         realized_pnl = self.reward\n",
        "        #         if self.cumulative_reward:\n",
        "        #             self.reward = self.reward / max(1, len(self.boughts))\n",
        "\n",
        "        #         if self.sudden_death * len(self.boughts) > self.reward:\n",
        "        #             self.nextDay = True\n",
        "\n",
        "        #         self.boughts = []\n",
        "        #         self.position = 0\n",
        "        #     elif self.budget > 0:\n",
        "        #         self.boughts.append(-1.0)\n",
        "        #         self.position = -1\n",
        "        #         self.budget -= 1.\n",
        "        \n",
        "        # else:\n",
        "        #     temp = temp * self.position\n",
        "        #     self.reward = self._get_holding_mutiplier(cum_reward, temp) * temp\n",
        "\n",
        "        # self.currentTargetIndex += 1\n",
        "    # def nextDay(self):\n",
        "    #     self.currentTargetIndex = self.scope - 1\n",
        "    #     self.currentDay = self.attack_sequence[self.day_index]\n",
        "    #     self.nextday = False\n",
        "    #     self.boughts = []\n",
        "    #     self.reward = 0.\n",
        "    #     self.position = 0\n",
        "    #     self.budget = self.budget*0 + 8.        # automation required # meaning power to buy x shares at current price\n",
        "    #     self._defineState()\n",
        "\n",
        "    #     return self.state\n",
        "\n",
        "        # tmpState['code'] = self.targetCode\n",
        "        # tmpState['day'] = self.currentDay\n",
        "        # tmpState['index'] = self.currentTargetIndex"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxfnNg8FJpUc"
      },
      "source": [
        "# parameters\n",
        "# epsilon = .5  # exploration\n",
        "# large_batches = True  # train in large batches for less I/O overhead.\n",
        "# min_epsilon = 0.1\n",
        "# batch_size = 16384 if large_batches else 64\n",
        "# discount = 0.9    # increasing future discount incentivise future P\n",
        "# epoch = 100\n",
        "# win_cnt = 0\n",
        "# train_cycle = 20000 if large_batches else 30\n",
        "# conter = 0\n",
        "\n",
        "# for e in range(epoch):\n",
        "#   loss = 0.\n",
        "#   day = 0\n",
        "#   game_over = False\n",
        "#   delay = True\n",
        "#   env.reset()\n",
        "\n",
        "#   while not game_over:\n",
        "    \n",
        "#     input_t = env.nextDay()\n",
        "#     day_end = False\n",
        "#     cumReward, realized_pnl, per_success, num_trades = 0, 0, 0, 0\n",
        "#     if delay and len(exp_replay.memory) > 40000:\n",
        "#       print('switching training delay off..')\n",
        "#       delay = False\n",
        "\n",
        "#     while not day_end:\n",
        "\n",
        "#       input_tm1 = input_t\n",
        "#       isRandom = False\n",
        "#       q = [None]\n",
        "\n",
        "#       # get next action\n",
        "#       if np.random.rand() <= epsilon or delay:\n",
        "#         action = np.random.randint(0, len(env.actions), size=1)[0]\n",
        "#         isRandom = True\n",
        "#       else:\n",
        "#         q = model_interface.predict(input_tm1)\n",
        "#         action = np.argmax(q[0])\n",
        "\n",
        "#       if np.nan in q:\n",
        "#         print(\"OCCUR NaN!!!\", str(counter), str(day), str(input_tm1['code']))\n",
        "#         # exit()\n",
        "\n",
        "#       # apply action, get rewards and new state\n",
        "#       input_t, reward, day_end, game_over, info, pnl, success = env.step(action)\n",
        "#       per_success += success > 0\n",
        "#       num_trades += abs(success)\n",
        "#       cumReward += reward\n",
        "#       realized_pnl += pnl\n",
        "\n",
        "#       # store experience\n",
        "#       exp_replay.remember([input_tm1,\n",
        "#                 action,\n",
        "#                 reward,\n",
        "#                 input_t],\n",
        "#                 day_end)\n",
        "\n",
        "#       # adapt model\n",
        "#       if counter % train_cycle == 0 and not delay:\n",
        "#         inputs, targets = exp_replay.get_offline_batch(env, model_interface, batch_size)\n",
        "#         if large_batches:\n",
        "#           loss = model_interface.fitModel(inputs, targets, 64, 10)\n",
        "#         else:\n",
        "#           loss = model_interface.batchTrain(inputs, targets)\n",
        "#         print('batch = ', str(counter // train_cycle), 'loss = ', str(loss))\n",
        "      \n",
        "#       counter += 1\n",
        "\n",
        "#     day += 1\n",
        "#     print('day = ', str(day), 'cumReward =', str(cumReward), 'pnl =', str(realized_pnl), 'percentage success', str(float(per_success) * 100 / max(num_trades, 1)))\n",
        "\n",
        "#     if cumReward > 0:\n",
        "#       win_cnt += 1\n",
        "    \n",
        "#     # kill this game randomly, expected 30 games in a row\n",
        "#     game_over = np.random.randint(30) == 0\n",
        "  \n",
        "#   print('switching game! epsilon=', str(epsilon))\n",
        "#   # print(\"Epoch {:03d}/{} | Loss {:.4f} | Win count {} | Epsilon {:.4f}\".format(e, epoch, loss, win_cnt, epsilon))\n",
        "#   # Save trained model weights and architecture, this will be used by the visualization code\n",
        "#   if day >= 800:\n",
        "#     epsilon = max(min_epsilon, epsilon * 0.99)\n",
        "#     model_interface.model.save_weights(\"model.h5\", overwrite=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}