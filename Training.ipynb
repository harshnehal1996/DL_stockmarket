{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "63m_7eC2keu7",
        "outputId": "470bda31-e562-48f4-bcf4-638b382e39d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t1m9592k0r2",
        "outputId": "19eecba9-09ff-4687-fb52-4547ff7d375f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd drive/'My Drive'/personal_projects/ta-lib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/personal_projects/ta-lib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqpFivmk6Zn",
        "outputId": "5e4f889a-3679-4bde-aa7e-77241ff37642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libta-lib0.\n",
            "(Reading database ... 144676 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) ...\n",
            "Selecting previously unselected package ta-lib0-dev.\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting ta-lib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/cf/681911aa31e04ba171ab4d523a412f4a746e30d3eacb1738799d181e028b/TA-Lib-0.4.19.tar.gz (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta-lib) (1.18.5)\n",
            "Building wheels for collected packages: ta-lib\n",
            "  Building wheel for ta-lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta-lib: filename=TA_Lib-0.4.19-cp36-cp36m-linux_x86_64.whl size=1437787 sha256=43a340456cedee6559b16c9e2f8811fadbee6ce6c0f636cd31ae97b7fab26b9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/f6/12/3d1ccd06caadd8fa47e016991dd0d27f1163bb260f1854e2ff\n",
            "Successfully built ta-lib\n",
            "Installing collected packages: ta-lib\n",
            "Successfully installed ta-lib-0.4.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZx3p_Yuk-yh",
        "outputId": "bd53cce5-da43-45c1-a757-c98df9e2d683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/personal_projects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_sEsbuW7Wgb",
        "outputId": "a0067215-e400-4981-a034-e7ac83af70e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Sep 29 12:21:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97r9YzRyf2cg"
      },
      "source": [
        "# common imports\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from talib import NATR, RSI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17GQES1HW1uz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Masking, Concatenate, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.layers.core import Activation, Dropout, Reshape\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from hyperas import optim\n",
        "# from hyperas.distributions import choice, uniform\n",
        "# from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZrw3vRZiozd"
      },
      "source": [
        "# model_interface.py\n",
        "\n",
        "# This is an abstract class. You need to implement yours.\n",
        "class AbstractModelBuilder(object):\n",
        "\n",
        "  def __init__(self, weights_path = None):\n",
        "    self.weights_path = weights_path\n",
        "    self.model = None\n",
        "\n",
        "  def loadModel(self):\n",
        "    weights_path = self.weights_path\n",
        "    self.model = self.buildModel()\n",
        "\n",
        "    if weights_path and os.path.isfile(weights_path):\n",
        "      try:\n",
        "        self.model.load_weights(weights_path)\n",
        "      except e:\n",
        "        print(e)\n",
        "\n",
        "  # You need to override this method.\n",
        "  def buildModel(self):\n",
        "    raise NotImplementedError(\"You need to implement your own model.\")\n",
        "\n",
        "  def predict(self, data):\n",
        "    raise NotImplementedError(\"You need to implement predictor function\")\n",
        "\n",
        "  def compile_model(self):\n",
        "    raise NotImplementedError(\"You need to implement this function\")\n",
        "\n",
        "  def trainModel(self, inputs, targets):\n",
        "    raise NotImplementedError(\"You need to implement this function\")\n",
        "  \n",
        "  def printSummary(self):\n",
        "    raise NotImplementedError(\"You need to implement this function\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxfFAS0EigJi"
      },
      "source": [
        "\n",
        "# do a cut off\n",
        "# unable to build (PnL transfer t to t+1)?\n",
        "# Doesn't work\n",
        "# class lstm_model(AbstractModelBuilder):\n",
        "  \n",
        "#   def buildModel(self):\n",
        "#       inputs = []\n",
        "    \n",
        "#       weekly_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "#       inputs.append(weekly_inputs)\n",
        "    \n",
        "#       encoder_inputs = Dense(8, activation='relu')(weekly_inputs)\n",
        "#       week_encoder = LSTM(16, return_state=True)\n",
        "#       encoder_outputs, state_h, state_c = week_encoder(encoder_inputs)\n",
        "#       encoder_states = [state_h, state_c]\n",
        "\n",
        "#       input_price_info = Input(shape=(None, 3))\n",
        "#       inputs.append(input_price_info)\n",
        "#       inputs_rsi_macd_schaff = Input(shape=(None, 4))\n",
        "#       inputs.append(inputs_rsi_macd_schaff)\n",
        "#       input_bolinger = Input(shape=(None, 3))\n",
        "#       inputs.append(input_bolinger)\n",
        "#       input_adx = Input(shape(None, 3))\n",
        "#       inputs.append(input_adx)\n",
        "#       input_one_hot = Input(shape(None, 8))\n",
        "#       inputs.append(input_one_hot)\n",
        "\n",
        "#       x1 = Dense(6, activation='leaky relu')(input_price_info)\n",
        "#       x2 = Dense(6, activation='leaky relu')(input_adx)\n",
        "#       x3 = Dense(6, activation='leaky relu')(input_bolinger)\n",
        "#       x4 = Dense(8, activation='leaky relu')(input_one_hot)\n",
        "\n",
        "#       feature_vector = Concatenate(axis=1)(x1, inputs_rsi_macd_schaff, x2, x3, x4)\n",
        "#       x = Dense(64, activation='relu')(feature_vector)\n",
        "#       embedding = Dropout(0.5)(x)\n",
        "\n",
        "#       layer_1 = LSTM(64, return_sequences=True)\n",
        "#       x = layer_1(embedding, initial_state=encoder_states)\n",
        "#       output_layer_1 = Activation('leaky relu')(x)\n",
        "\n",
        "#       layer_2 = LSTM(32, return_sequences=True)\n",
        "#       x = layer_2(output_layer_1)\n",
        "#       output_layer_2 = Activation('leaky relu')(x)\n",
        "\n",
        "#       final_dense = Dense(3, activation='linear')\n",
        "#       final_output = final_dense(output_layer_2)\n",
        "\n",
        "#       model = Model(inputs, final_output)\n",
        "#       return model\n",
        "\n",
        "# Take past 30 timesteps one hot veectors then use cnn and give it to the neural net \n",
        "\n",
        "\n",
        "class cnn_model(AbstractModelBuilder):\n",
        "\n",
        "  def buildModel(self):\n",
        "    hourly_data = Input(shape=(1, 30, 3))\n",
        "\n",
        "    x = Conv2D(2, (1,3), activation='relu')(hourly_data)\n",
        "    x = MaxPooling2D(pool_size=(1,2))(x)\n",
        "    x = Conv2D(4, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2))(x)\n",
        "    x = Conv2D(4, (1,3), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x0 = Dense(8, activation='relu')(x)\n",
        "\n",
        "    input_PnL = Input(shape=(9,))\n",
        "    x1 = Dense(4)(input_PnL)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "\n",
        "    # scaling\n",
        "    input_scale = Input(shape=(30,))\n",
        "    x = Dense(4, activation='relu')(input_scale)\n",
        "    x_scale = Dense(1)(x)\n",
        "\n",
        "    inputs = [hourly_data, input_PnL, input_scale]\n",
        "    input_price_info = Input(shape=(1, 60, 2))\n",
        "    inputs.append(input_price_info)\n",
        "    input_indicators = Input(shape=(1, 60, 5))\n",
        "    inputs.append(input_indicators)\n",
        "    input_bolinger = Input(shape=(4, 60, 1))\n",
        "    inputs.append(input_bolinger)\n",
        "    input_adx = Input(shape=(1, 60, 3))\n",
        "    inputs.append(input_adx)\n",
        "    input_nb = Input(shape=(1, 30, 2))\n",
        "    inputs.append(input_nb)\n",
        "    input_onehot = Input(shape=(10, 30, 1))\n",
        "    inputs.append(input_onehot)\n",
        "\n",
        "    # price\n",
        "    x = Conv2D(5, (1,3), activation='relu')(input_price_info)\n",
        "    x = Conv2D(8, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(12, (1,3), activation='relu')(x)\n",
        "    x = Conv2D(14, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(16, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(25, activation='relu')(x)\n",
        "    x_price = Dropout(0.3)(x)\n",
        "\n",
        "    # nifty bank\n",
        "    x = Conv2D(3, (1,3), activation='relu')(input_nb)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(4, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(5, (1,3), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x_nb = Dense(6)(x)\n",
        "\n",
        "    # adx\n",
        "    x = Dense(1)(input_adx)\n",
        "    x = LeakyReLU()(x)\n",
        "    indicators = Concatenate(axis=3)([input_indicators, x, input_adx[:,:,:,1:]])\n",
        "\n",
        "    # bolinger\n",
        "    x = Conv2D(3, (4,3), activation='relu')(input_bolinger)\n",
        "    x = Conv2D(4, (1,5), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(5, (1,7), activation='relu')(x)\n",
        "    x = Conv2D(6, (1,10), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(10, activation='relu')(x)\n",
        "    x_bolinger = Dropout(0.3)(x)\n",
        "\n",
        "    # onehot\n",
        "    x = Conv2D(8, (8,3), activation='relu')(input_onehot)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(10, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(12, (1,5), activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(10, 'relu')(x)\n",
        "    x_onehot = Dropout(0.2)(x)\n",
        "\n",
        "    #indicators\n",
        "    x = Conv2D(8, (1,3), activation='relu')(indicators)\n",
        "    x = Conv2D(12, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x) \n",
        "    x = Conv2D(12, (1,3), activation='relu')(x)\n",
        "    x = Conv2D(16, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Conv2D(20, (1,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(1,2), strides=(1,2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(20, activation='relu')(x)\n",
        "    x_indicators = Dropout(0.3)(x)\n",
        "\n",
        "    # concatenate and combine all the feature_vectors\n",
        "    x = Concatenate(axis=1)([x_price, x_bolinger, x_indicators, x_onehot, x_nb])\n",
        "    x = Dense(30)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # concatenate context\n",
        "    x = Concatenate(axis=1)([x, x1, x0])\n",
        "    x = Dense(23)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Dense(12)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    # Experiment with tanh too\n",
        "    x = Dense(3, activation='linear')(x)\n",
        "\n",
        "    # provide scaling\n",
        "    # possible to force x_scale > 0 ? \n",
        "    final_output = x * x_scale\n",
        "\n",
        "    # num trainable params large ~ 20k ? \n",
        "    model = Model(inputs, final_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  def compile_model(self):\n",
        "    self.model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "  def predict(self, data):   \n",
        "    return self.model.predict([data['hourly_data'],   \n",
        "                   data['portfolio'],\n",
        "                   data['scale'], \n",
        "                   data['price_curve'],\n",
        "                   data['indicators'],\n",
        "                   data['bolinger'],\n",
        "                   data['adx'],\n",
        "                   data['nb'],\n",
        "                   data['onehot']])\n",
        "\n",
        "\n",
        "  def trainModel(self, data, targets):\n",
        "    return self.model.train_on_batch([data['hourly_data'],    \n",
        "                      data['portfolio'],\n",
        "                      data['scale'], \n",
        "                      data['price_curve'],\n",
        "                      data['indicators'],\n",
        "                      data['bolinger'],\n",
        "                      data['adx'],\n",
        "                      data['nb'],\n",
        "                      data['onehot']],\n",
        "                      targets)\n",
        "  \n",
        "  def printSummary(self):\n",
        "    print(self.model.summary())"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxPFqMFmP9yO"
      },
      "source": [
        "# Improvements\n",
        "# prioritized Experience Replay\n",
        "# Dual Target Network\n",
        "# Better Optimzation techique\n",
        "class ExperienceReplay(object):\n",
        "  \n",
        "  def __init__(self, max_memory=100000, discount=.9):\n",
        "    self.max_memory = max_memory\n",
        "    self.count_range = {}\n",
        "    self.memory = list()\n",
        "    self.discount = discount\n",
        "    self.record_sort = False\n",
        "\n",
        "  def remember(self, states, game_over):\n",
        "    # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
        "    self.memory.append([states, game_over, 0])\n",
        "    if self.record_sort:\n",
        "      if self.count_range.__contains__(0):\n",
        "        l, r = self.count_range[0]\n",
        "        self.count_range[0] = (l, r + 1)\n",
        "      else:\n",
        "        self.count_range[0] = (len(self.memory) - 1, len(self.memory) - 1)\n",
        "    \n",
        "    if len(self.memory) > self.max_memory:\n",
        "      del self.memory[:500]\n",
        "\n",
        "  # keep record sorted\n",
        "  def _adjust_record(self, idx):\n",
        "    if not self.record_sort:\n",
        "      return\n",
        "\n",
        "    old_count = self.memory[idx][2]\n",
        "    self.memory[idx][2] = old_count + 1\n",
        "    old_l,old_r = self.count_range[old_count]\n",
        "  \n",
        "    if idx != old_l:\n",
        "      temp = self.memory[idx]\n",
        "      self.memory[idx] = self.memory[old_l]\n",
        "      self.memory[old_l] = temp\n",
        "\n",
        "    if self.count_range.__contains__(old_count + 1):\n",
        "      new_l,new_r = self.count_range[old_count + 1]\n",
        "      self.count_range[old_count + 1] = (new_l, new_r + 1)\n",
        "    else:\n",
        "      self.count_range[old_count + 1] = (old_l, old_l)\n",
        "\n",
        "    if old_l + 1 > old_r:\n",
        "      del self.count_range[old_count - 1]\n",
        "    else:\n",
        "      self.count_range[old_count - 1] = (old_l + 1, old_r)\n",
        "\n",
        "  def get_batch(self, env, model_interface, batch_size=64):\n",
        "    len_memory = len(self.memory)\n",
        "    num_actions = model_interface.model.output_shape[-1]\n",
        "    inputs = {}\n",
        "    price_curve, indicators, bolinger, adx, nb, onehot, portfolio, hourly_data, scale_data = [], [], [], [], [], [], [], [], []\n",
        "\n",
        "    targets = np.zeros((min(len_memory, batch_size), num_actions))\n",
        "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):\n",
        "      pnl_state_t, action_t, reward_t, pnl_state_tp1, code, day, index = self.memory[idx][0]\n",
        "      self._adjust_record(idx)\n",
        "      state_t = env.generate_data(code, day, index)\n",
        "      state_t['portfolio'] = pnl_state_t\n",
        "      game_over = self.memory[idx][1]\n",
        "      targets[i] = model_interface.predict(state_t)[0]\n",
        "      if not game_over:\n",
        "        state_tp1 = env.generate_data(code, day, index + 1)\n",
        "        state_tp1['portfolio'] = pnl_state_tp1\n",
        "        Q_sa = np.max(model_interface.predict(state_tp1)[0])\n",
        "        targets[i, action_t] = reward_t + self.discount * Q_sa\n",
        "      else:\n",
        "        targets[i, action_t] = reward_t\n",
        "\n",
        "      price_curve.append(np.squeeze(state_t['price_curve'], axis=0))\n",
        "      indicators.append(np.squeeze(state_t['indicators'], axis=0))\n",
        "      bolinger.append(np.squeeze(state_t['bolinger'], axis=0))\n",
        "      adx.append(np.squeeze(state_t['adx'], axis=0))\n",
        "      onehot.append(np.squeeze(state_t['onehot'], axis=0))\n",
        "      nb.append(np.squeeze(state_t['nb'], axis=0))\n",
        "      portfolio.append(np.squeeze(state_t['portfolio'], axis=0))\n",
        "      hourly_data.append(np.squeeze(state_t['hourly_data'], axis=0))\n",
        "      scale_data.append(np.squeeze(state_t['scale'], axis=0))\n",
        "\n",
        "\n",
        "    inputs['price_curve'] = np.array(price_curve)\n",
        "    inputs['indicators'] = np.array(indicators)\n",
        "    inputs['bolinger'] = np.array(bolinger)\n",
        "    inputs['adx'] = np.array(adx)\n",
        "    inputs['onehot'] = np.array(onehot)\n",
        "    inputs['portfolio'] = np.array(portfolio)\n",
        "    inputs['nb'] = np.array(nb)\n",
        "    inputs['scale'] = np.array(scale_data)\n",
        "    inputs['hourly_data'] = np.array(hourly_data)\n",
        "\n",
        "    return inputs, targets\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YbXipbUeJD"
      },
      "source": [
        "path_list = ['BAJFINANCE/processed_data', 'AXISBANK/processed_data', 'HDFC/processed_data', 'HDFCBANK/processed_data', 'ICICIBANK/processed_data']\n",
        "X_price_info, X_indicators, X_adx, X_bolinger, X_onehot, X_nb = [{} for _ in range(6)]\n",
        "X_avg_val, X_high_val, X_low_val, X_close_val = [{} for _ in range(4)]\n",
        "\n",
        "code = 0\n",
        "for path in path_list:\n",
        "  if not os.path.exists(path):\n",
        "    print('path not found', path)\n",
        "  else:\n",
        "    f = open(path, 'rb')\n",
        "    obj = pickle.load(f)\n",
        "    X_price_info[code] = obj['X_price_info']\n",
        "    X_indicators[code] = obj['X_indicators']\n",
        "    X_adx[code] = obj['X_adx']\n",
        "    X_bolinger[code] = obj['X_bolinger']\n",
        "    X_onehot[code] = obj['X_onehot']\n",
        "    X_nb[code] = obj['X_nb']\n",
        "    X_avg_val[code] = obj['avg_val']\n",
        "    X_high_val[code] = obj['high_val']\n",
        "    X_low_val[code] = obj['low_val']\n",
        "    X_close_val[code] = obj['close_val']\n",
        "    code += 1\n",
        "    f.close()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVHAJ_URFCZz"
      },
      "source": [
        "# parameters\n",
        "epsilon = .5  # exploration\n",
        "min_epsilon = 0.1\n",
        "epoch = 2\n",
        "max_memory = 5000\n",
        "batch_size = 64\n",
        "scope = 60\n",
        "discount = 0.9    # increasing future discount incentivise future P"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMwPvUE5buhy"
      },
      "source": [
        "env = MarketEnv(X_price_info,\n",
        "        X_indicators,\n",
        "        X_bolinger,\n",
        "        X_adx,\n",
        "        X_onehot,\n",
        "        X_nb,\n",
        "        X_avg_val,\n",
        "        X_close_val,\n",
        "        X_high_val,\n",
        "        X_low_val,\n",
        "        scope,\n",
        "        sudden_death = -1.0)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_Q6rVQVwys"
      },
      "source": [
        "# from keras.optimizers import SGD\n",
        "modelFilename='cnn_model.h5'\n",
        "model_interface = cnn_model(modelFilename)\n",
        "model_interface.loadModel()\n",
        "model_interface.compile_model()\n",
        "\n",
        "# sgd = SGD(lr = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "# need a better optimization/random optimization techique.\n",
        "# maybe try Particle swarm or genectic algorithm"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YLQcjc4XN4Q"
      },
      "source": [
        "# Initialize experience replay object\n",
        "exp_replay = ExperienceReplay()\n",
        "# Train\n",
        "win_cnt = 0\n",
        "train_cycle = 30"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm1tOy_W80fV",
        "outputId": "55a566fd-5428-4693-c8b7-daff40a5fc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "exp_replay"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.ExperienceReplay at 0x7f475066a860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA0yR1fUadZd",
        "outputId": "55c202eb-8558-4066-999f-e002ddd7ede9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "env"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MarketEnv at 0x7f475066a128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1cdge8Us6W",
        "outputId": "e8686407-63bb-4e17-b837-229f99ebc56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_interface.predict(input_t)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.4604977e-04,  2.3279474e-05, -7.8652592e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3fojPEizQ0B",
        "outputId": "b442ea01-02be-4391-e539-62574716ef1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_interface.predict(input_tm1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.7858001e-04,  3.6566528e-06, -9.8831340e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJVFDDRiOJlZ"
      },
      "source": [
        "model_interface.model.save_weights(\"model_checkpoint_day_155.h5\" if modelFilename == None else modelFilename, overwrite=True)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5m1mJfYeDu",
        "outputId": "41a1dc85-84e0-4339-89cb-071c6ae482dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch = 1\n",
        "for e in range(epoch):\n",
        "  loss = 0.\n",
        "  day = 0\n",
        "  game_over = False\n",
        "  delay = True\n",
        "  env.reset()\n",
        "\n",
        "  while not game_over:\n",
        "    \n",
        "    input_t = env.nextDay()\n",
        "    day_end = False\n",
        "    counter = 0\n",
        "    cumReward = 0\n",
        "    realized_pnl = 0\n",
        "    if delay and len(exp_replay.memory) > 30000:\n",
        "      print('switching training delay off..')\n",
        "      delay = False\n",
        "\n",
        "    while not day_end:\n",
        "\n",
        "      input_tm1 = input_t\n",
        "      isRandom = False\n",
        "      q = [None]\n",
        "\n",
        "      # get next action\n",
        "      if np.random.rand() <= epsilon or delay:\n",
        "        action = np.random.randint(0, len(env.actions), size=1)[0]\n",
        "        isRandom = True\n",
        "      else:\n",
        "        q = model_interface.predict(input_tm1)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "      if np.nan in q:\n",
        "        print(\"OCCUR NaN!!!\", str(counter), str(day), str(input_tm1['code']))\n",
        "        # exit()\n",
        "\n",
        "      # apply action, get rewards and new state\n",
        "      input_t, reward, day_end, game_over, info, pnl = env.step(action)\n",
        "      cumReward += reward\n",
        "      realized_pnl += pnl\n",
        "\n",
        "      # store experience\n",
        "      exp_replay.remember([input_tm1['portfolio'],\n",
        "                action,\n",
        "                reward,\n",
        "                input_t['portfolio'],\n",
        "                input_tm1['code'],\n",
        "                input_tm1['day'],\n",
        "                input_tm1['index']],\n",
        "                day_end)\n",
        "\n",
        "      # adapt model\n",
        "      if counter % train_cycle == 0 and not delay:\n",
        "        inputs, targets = exp_replay.get_batch(env, model_interface)\n",
        "        loss = model_interface.trainModel(inputs, targets)\n",
        "        print('batch = ', str(counter // train_cycle), 'loss = ', str(loss))\n",
        "      \n",
        "      counter += 1\n",
        "\n",
        "    day += 1\n",
        "    print('day = ', str(day), 'cumReward =', str(cumReward), 'pnl =', str(realized_pnl))\n",
        "\n",
        "    if cumReward > 0:\n",
        "      win_cnt += 1\n",
        "\n",
        "  print(\"Epoch {:03d}/{} | Loss {:.4f} | Win count {} | Epsilon {:.4f}\".format(e, epoch, loss, win_cnt, epsilon))\n",
        "  # Save trained model weights and architecture, this will be used by the visualization code\n",
        "  model_interface.model.save_weights(\"model.h5\" if modelFilename == None else modelFilename, overwrite=True)\n",
        "  epsilon = max(min_epsilon, epsilon * 0.99)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "day =  1 cumReward = 0.013450827067096383 pnl = 0.004149384049375149\n",
            "day =  2 cumReward = -0.02846824665938035 pnl = -0.03248094130769108\n",
            "day =  3 cumReward = 0.013805230808175552 pnl = 0.008900586010251121\n",
            "day =  4 cumReward = 0.08823392060068269 pnl = 0.05799911419642656\n",
            "day =  5 cumReward = -0.008236795590402482 pnl = -0.012385912192470205\n",
            "day =  6 cumReward = 0.01805715597449166 pnl = 0.0024780707601942398\n",
            "day =  7 cumReward = 0.054442825751373936 pnl = 0.03271713296647727\n",
            "day =  8 cumReward = 0.0068140603037950675 pnl = -0.0031255762698507716\n",
            "day =  9 cumReward = 0.00881053589239995 pnl = 0.0020726299111311075\n",
            "day =  10 cumReward = 0.03728889865915179 pnl = 0.018338434667893866\n",
            "day =  11 cumReward = 0.03400846465400684 pnl = 0.019372358253247235\n",
            "day =  12 cumReward = 0.04391469264112428 pnl = 0.025435898318079797\n",
            "day =  13 cumReward = -0.012538540081231538 pnl = -0.019479459872003724\n",
            "day =  14 cumReward = -0.013837073573874325 pnl = -0.01864241897127028\n",
            "day =  15 cumReward = 0.030877793227920083 pnl = 0.007334562328398131\n",
            "day =  16 cumReward = 0.04349273336986157 pnl = 0.013251284823104892\n",
            "day =  17 cumReward = 0.021978531762772244 pnl = 0.013487738125023019\n",
            "day =  18 cumReward = -0.004122527045277015 pnl = -0.019138139179994185\n",
            "day =  19 cumReward = 0.02375889196633034 pnl = 0.014717423338989621\n",
            "day =  20 cumReward = 0.035018857069155675 pnl = 0.016659700992109006\n",
            "day =  21 cumReward = 0.04044156995232183 pnl = 0.02122271125330144\n",
            "day =  22 cumReward = -0.020900258427449893 pnl = -0.022560512628196583\n",
            "day =  23 cumReward = 0.07663820782314001 pnl = 0.043320770141036635\n",
            "day =  24 cumReward = 0.01750238175284391 pnl = 0.0012818064439632515\n",
            "day =  25 cumReward = 0.04645785282962833 pnl = 0.019945963710831127\n",
            "day =  26 cumReward = 0.08869972032659895 pnl = 0.0419212545089257\n",
            "day =  27 cumReward = 0.007605125773163655 pnl = 0.0014433637751467554\n",
            "day =  28 cumReward = 0.008521942483561485 pnl = -0.0008384409190674846\n",
            "day =  29 cumReward = 0.015846727051492385 pnl = 3.246842447701681e-05\n",
            "day =  30 cumReward = 0.0763200874974022 pnl = 0.03651450656943811\n",
            "day =  31 cumReward = -0.0015640342175719472 pnl = -0.007750187390838881\n",
            "day =  32 cumReward = 0.0031536693087443747 pnl = -0.008619451102739317\n",
            "day =  33 cumReward = -0.0016953866321640543 pnl = -0.011461590361845464\n",
            "day =  34 cumReward = 0.014756311874479197 pnl = 0.0035499594116731314\n",
            "day =  35 cumReward = -0.005683462216298935 pnl = -0.010749364597423372\n",
            "day =  36 cumReward = -0.02288179469557485 pnl = -0.024304863825627132\n",
            "day =  37 cumReward = 0.0362363996245946 pnl = 0.02122031278292702\n",
            "day =  38 cumReward = 0.03856484945220958 pnl = 0.01717732623802759\n",
            "day =  39 cumReward = 0.028150553498661333 pnl = 0.012857354915546515\n",
            "day =  40 cumReward = 0.03483222307277743 pnl = 0.021532205400167315\n",
            "day =  41 cumReward = -0.012136440128022742 pnl = -0.019034823752620444\n",
            "day =  42 cumReward = 0.018935087009766077 pnl = 0.006333345922942546\n",
            "day =  43 cumReward = -0.0011908309953812758 pnl = -0.009377434239704718\n",
            "day =  44 cumReward = -0.004423169121028258 pnl = -0.006351065155196012\n",
            "day =  45 cumReward = 0.017463032845427152 pnl = 0.007006909014728224\n",
            "day =  46 cumReward = 0.04086613411079398 pnl = 0.018698854001938048\n",
            "day =  47 cumReward = 0.025729141732669974 pnl = 0.013844195798989656\n",
            "day =  48 cumReward = -0.0009542215364086913 pnl = -0.030207814804513733\n",
            "day =  49 cumReward = 0.04787663553172045 pnl = 0.015545224684473613\n",
            "day =  50 cumReward = 2.5861032227598156e-05 pnl = -0.003521707007122843\n",
            "day =  51 cumReward = -0.009254765921900455 pnl = -0.0224419914321643\n",
            "day =  52 cumReward = 0.06583853066989107 pnl = 0.015292918506602526\n",
            "day =  53 cumReward = 0.014899525904766238 pnl = 0.003740072920419002\n",
            "day =  54 cumReward = 0.11505201902218189 pnl = 0.06456162119182418\n",
            "day =  55 cumReward = -0.03314286448343369 pnl = -0.03401785530436019\n",
            "day =  56 cumReward = -0.01973802359624009 pnl = -0.029805721910337923\n",
            "day =  57 cumReward = 0.026811789321937923 pnl = 0.00946000348215792\n",
            "day =  58 cumReward = -0.0038320012700761292 pnl = -0.02493644122013572\n",
            "day =  59 cumReward = 0.06494332224400622 pnl = 0.021372893967201434\n",
            "day =  60 cumReward = -0.010997786460389852 pnl = -0.012922499636074725\n",
            "day =  61 cumReward = 0.009167238774312192 pnl = 0.008640360271649716\n",
            "day =  62 cumReward = 0.020362125580721417 pnl = 0.005330817329923465\n",
            "day =  63 cumReward = 0.02070120451727568 pnl = 0.011443562207373081\n",
            "day =  64 cumReward = -0.02883228097794828 pnl = -0.030153513709811786\n",
            "day =  65 cumReward = -0.03145587070435063 pnl = -0.05391792198134959\n",
            "day =  66 cumReward = 0.0165714889294702 pnl = 0.006358170564817023\n",
            "day =  67 cumReward = 0.010464202207702165 pnl = 0.00567998691669358\n",
            "day =  68 cumReward = 0.012351106437290633 pnl = -0.008139391585289535\n",
            "day =  69 cumReward = 0.06165317771163813 pnl = 0.04404002585268074\n",
            "day =  70 cumReward = 0.0034690542940878756 pnl = -0.003933207130325522\n",
            "day =  71 cumReward = 0.09623720755988589 pnl = 0.054648050546381444\n",
            "day =  72 cumReward = 0.026260986028316592 pnl = 0.008413665926290337\n",
            "day =  73 cumReward = -0.05674471544375395 pnl = -0.0691466568921959\n",
            "day =  74 cumReward = 0.04788815198107569 pnl = 0.028778674871345222\n",
            "day =  75 cumReward = -0.00023682229465523622 pnl = -0.007715693627734499\n",
            "day =  76 cumReward = 0.01290450390743773 pnl = -0.00184398381868367\n",
            "day =  77 cumReward = 0.005565345751873862 pnl = 0.0017510015635767706\n",
            "day =  78 cumReward = 0.020201157668347156 pnl = 0.007606195477838695\n",
            "day =  79 cumReward = 0.08908096000000584 pnl = 0.05389354684915837\n",
            "day =  80 cumReward = 0.0026485709002602702 pnl = -0.008709453207491613\n",
            "day =  81 cumReward = 0.019803972711031348 pnl = 0.013460771174987007\n",
            "day =  82 cumReward = 0.023636092649718443 pnl = 0.008532440945723252\n",
            "day =  83 cumReward = 0.03254572101512053 pnl = 0.01862383000957768\n",
            "day =  84 cumReward = 0.04284321592449602 pnl = 0.024171013412708997\n",
            "day =  85 cumReward = 0.020829745335691846 pnl = 0.006957993735887125\n",
            "day =  86 cumReward = 0.018731622103802435 pnl = -0.000528080247050311\n",
            "day =  87 cumReward = 0.06096492644134105 pnl = 0.03721336776090689\n",
            "day =  88 cumReward = 0.023644239249534 pnl = 0.010397196807224396\n",
            "day =  89 cumReward = 0.045303878826242386 pnl = 0.02129167605875526\n",
            "day =  90 cumReward = 0.07989166726507775 pnl = 0.045854174514569634\n",
            "day =  91 cumReward = 0.09117977665697725 pnl = 0.05840153399802295\n",
            "day =  92 cumReward = 0.013605110247972903 pnl = 0.01249239767817667\n",
            "day =  93 cumReward = -0.03858390129583052 pnl = -0.04191052069115464\n",
            "day =  94 cumReward = 0.00556936830843589 pnl = 0.0010722384508564664\n",
            "day =  95 cumReward = 0.01779405040162939 pnl = -0.005118118526166171\n",
            "day =  96 cumReward = 0.00496864133856563 pnl = -0.002532528290687175\n",
            "day =  97 cumReward = 0.01819914245213931 pnl = -0.0013632777346689329\n",
            "day =  98 cumReward = 0.010019918037919144 pnl = -0.0026747597936173095\n",
            "day =  99 cumReward = -0.05011038947694644 pnl = -0.060449596130375594\n",
            "day =  100 cumReward = 0.008949903620743913 pnl = 0.0025007264387548744\n",
            "day =  101 cumReward = 0.0038329409835242845 pnl = -0.00476182996735075\n",
            "day =  102 cumReward = 0.14276975816478488 pnl = 0.08466080208487303\n",
            "day =  103 cumReward = 0.0019434933447946392 pnl = -0.00886606060769568\n",
            "day =  104 cumReward = 0.034171374595763256 pnl = 0.010688997161484526\n",
            "day =  105 cumReward = 0.04098966443782011 pnl = 0.02084557274437282\n",
            "switching training delay off..\n",
            "batch =  0 loss =  4.0337657992495224e-05\n",
            "batch =  1 loss =  2.581326407380402e-05\n",
            "batch =  2 loss =  2.4962728275568224e-05\n",
            "batch =  3 loss =  2.7414396754465997e-05\n",
            "batch =  4 loss =  2.665771171450615e-05\n",
            "batch =  5 loss =  2.5258032110286877e-05\n",
            "batch =  6 loss =  2.5513038053759374e-05\n",
            "batch =  7 loss =  2.1337782527552918e-05\n",
            "batch =  8 loss =  2.5480778276687488e-05\n",
            "batch =  9 loss =  1.4036196262168232e-05\n",
            "day =  106 cumReward = 0.10887800551936479 pnl = 0.10423682676552737\n",
            "batch =  0 loss =  2.0512958144536242e-05\n",
            "batch =  1 loss =  1.9195713321096264e-05\n",
            "batch =  2 loss =  1.3297680197865702e-05\n",
            "batch =  3 loss =  1.46612064781948e-05\n",
            "batch =  4 loss =  1.4095903679844923e-05\n",
            "batch =  5 loss =  1.2547855476441327e-05\n",
            "batch =  6 loss =  1.1327929314575158e-05\n",
            "batch =  7 loss =  1.6640340618323535e-05\n",
            "batch =  8 loss =  1.2032159247610252e-05\n",
            "batch =  9 loss =  1.000217434921069e-05\n",
            "day =  107 cumReward = -0.028326372457959267 pnl = -0.03484946075128392\n",
            "batch =  0 loss =  1.3166375538276043e-05\n",
            "batch =  1 loss =  7.112791536201257e-06\n",
            "batch =  2 loss =  7.551901944680139e-06\n",
            "batch =  3 loss =  6.399420271918643e-06\n",
            "batch =  4 loss =  8.708979294169694e-06\n",
            "batch =  5 loss =  9.248115929949563e-06\n",
            "batch =  6 loss =  1.0739606295828708e-05\n",
            "batch =  7 loss =  1.1453284059825819e-05\n",
            "batch =  8 loss =  8.799628631095402e-06\n",
            "batch =  9 loss =  7.568232831545174e-06\n",
            "day =  108 cumReward = 0.002667894549945549 pnl = -0.0012796092155885619\n",
            "batch =  0 loss =  8.423450708505698e-06\n",
            "batch =  1 loss =  5.815618806082057e-06\n",
            "batch =  2 loss =  9.478617357672192e-06\n",
            "batch =  3 loss =  3.479670567685389e-06\n",
            "batch =  4 loss =  5.372113264456857e-06\n",
            "batch =  5 loss =  4.624533175956458e-06\n",
            "batch =  6 loss =  6.204280907695647e-06\n",
            "batch =  7 loss =  5.097708253742894e-06\n",
            "batch =  8 loss =  5.872104793525068e-06\n",
            "batch =  9 loss =  7.713824743404984e-06\n",
            "day =  109 cumReward = -0.010329884390691076 pnl = -0.0178412557399259\n",
            "batch =  0 loss =  3.960265530622564e-06\n",
            "batch =  1 loss =  3.711219960678136e-06\n",
            "batch =  2 loss =  4.951194114255486e-06\n",
            "batch =  3 loss =  3.822868166025728e-06\n",
            "batch =  4 loss =  5.291602064971812e-06\n",
            "batch =  5 loss =  5.790752311440883e-06\n",
            "batch =  6 loss =  7.95187588664703e-06\n",
            "batch =  7 loss =  3.0257147045631427e-06\n",
            "batch =  8 loss =  4.074413482157979e-06\n",
            "batch =  9 loss =  2.5723293219925836e-06\n",
            "day =  110 cumReward = -0.021400332880408424 pnl = -0.036210974243995886\n",
            "batch =  0 loss =  5.9548920035013e-06\n",
            "batch =  1 loss =  3.0131338917271933e-06\n",
            "batch =  2 loss =  3.7888182760070777e-06\n",
            "batch =  3 loss =  3.6876133435725933e-06\n",
            "batch =  4 loss =  3.6226294923835667e-06\n",
            "batch =  5 loss =  4.152813289692858e-06\n",
            "batch =  6 loss =  3.556410320015857e-06\n",
            "batch =  7 loss =  2.741350499491091e-06\n",
            "batch =  8 loss =  3.5208981898904312e-06\n",
            "batch =  9 loss =  2.0379418401716975e-06\n",
            "day =  111 cumReward = 0.10377869645769204 pnl = 0.0796643149798385\n",
            "batch =  0 loss =  3.219174232071964e-06\n",
            "batch =  1 loss =  1.7205090898642084e-06\n",
            "batch =  2 loss =  3.0115879781078547e-06\n",
            "batch =  3 loss =  2.920776523751556e-06\n",
            "batch =  4 loss =  4.471547981665935e-06\n",
            "batch =  5 loss =  2.244456482003443e-06\n",
            "batch =  6 loss =  3.0235396479838528e-06\n",
            "batch =  7 loss =  2.1091473172418773e-06\n",
            "batch =  8 loss =  3.4498857530707028e-06\n",
            "batch =  9 loss =  3.003627625730587e-06\n",
            "day =  112 cumReward = -0.02526236478432045 pnl = -0.03066377542969323\n",
            "batch =  0 loss =  2.6781976885104086e-06\n",
            "batch =  1 loss =  7.013852155068889e-06\n",
            "batch =  2 loss =  2.7381065592635423e-06\n",
            "batch =  3 loss =  4.8846241043065675e-06\n",
            "batch =  4 loss =  9.106855941354297e-06\n",
            "batch =  5 loss =  2.6747707124741282e-06\n",
            "batch =  6 loss =  3.7650222566298908e-06\n",
            "batch =  7 loss =  2.5866333999147173e-06\n",
            "batch =  8 loss =  2.0778011275979225e-06\n",
            "batch =  9 loss =  2.706043460420915e-06\n",
            "day =  113 cumReward = 0.03063153302673921 pnl = 0.01716271619433496\n",
            "batch =  0 loss =  1.5223204172798432e-06\n",
            "batch =  1 loss =  2.8728372853947803e-06\n",
            "batch =  2 loss =  4.206048288324382e-06\n",
            "batch =  3 loss =  1.6113060610223329e-06\n",
            "batch =  4 loss =  3.317739583508228e-06\n",
            "batch =  5 loss =  1.663679313423927e-06\n",
            "batch =  6 loss =  2.1893231405556435e-06\n",
            "batch =  7 loss =  3.0842463729641167e-06\n",
            "batch =  8 loss =  2.0107927412027493e-06\n",
            "batch =  9 loss =  2.398904598521767e-06\n",
            "day =  114 cumReward = -0.0288622704666381 pnl = -0.05209137411234899\n",
            "batch =  0 loss =  2.5320200620626565e-06\n",
            "batch =  1 loss =  2.4739551918173674e-06\n",
            "batch =  2 loss =  1.5814946436876198e-06\n",
            "batch =  3 loss =  2.4518801637896104e-06\n",
            "batch =  4 loss =  1.3627753787659458e-06\n",
            "batch =  5 loss =  3.2815141821629368e-06\n",
            "batch =  6 loss =  1.556023789817118e-06\n",
            "batch =  7 loss =  1.8009719724432216e-06\n",
            "batch =  8 loss =  1.1934512258449104e-05\n",
            "batch =  9 loss =  2.180948740715394e-06\n",
            "day =  115 cumReward = 0.013183052013041228 pnl = -0.005578709754185063\n",
            "batch =  0 loss =  1.649053501751041e-06\n",
            "batch =  1 loss =  1.6646990843582898e-06\n",
            "batch =  2 loss =  9.10450410174235e-07\n",
            "batch =  3 loss =  1.3144400327291805e-05\n",
            "batch =  4 loss =  1.6124859030242078e-06\n",
            "batch =  5 loss =  5.126576070324518e-06\n",
            "batch =  6 loss =  1.5171519862633431e-06\n",
            "batch =  7 loss =  2.304363988514524e-06\n",
            "batch =  8 loss =  1.2065768260072218e-06\n",
            "batch =  9 loss =  1.6193561123145628e-06\n",
            "day =  116 cumReward = 0.023961863163310136 pnl = 0.02183061335946923\n",
            "batch =  0 loss =  1.260667659153114e-06\n",
            "batch =  1 loss =  4.760565843753284e-06\n",
            "batch =  2 loss =  2.8814238248742186e-06\n",
            "batch =  3 loss =  2.6088407594215823e-06\n",
            "batch =  4 loss =  7.85417910265096e-07\n",
            "batch =  5 loss =  3.474047161944327e-06\n",
            "batch =  6 loss =  9.885847020996152e-07\n",
            "batch =  7 loss =  9.802095064515015e-07\n",
            "batch =  8 loss =  1.05679555417737e-06\n",
            "batch =  9 loss =  2.669173909453093e-06\n",
            "day =  117 cumReward = 0.027516869029752776 pnl = 0.010552722255654512\n",
            "batch =  0 loss =  1.5111959328351077e-06\n",
            "batch =  1 loss =  1.3022406619711546e-06\n",
            "batch =  2 loss =  1.940441052283859e-06\n",
            "batch =  3 loss =  2.393693648627959e-06\n",
            "batch =  4 loss =  1.167243908639648e-06\n",
            "batch =  5 loss =  2.082689206872601e-06\n",
            "batch =  6 loss =  1.1492917337818653e-06\n",
            "batch =  7 loss =  1.5705921896369546e-06\n",
            "batch =  8 loss =  2.18521222450363e-06\n",
            "batch =  9 loss =  1.4153913525660755e-06\n",
            "day =  118 cumReward = 0.011175665657219128 pnl = -0.00013413785723592753\n",
            "batch =  0 loss =  1.1243145081607508e-06\n",
            "batch =  1 loss =  2.8880776881123893e-06\n",
            "batch =  2 loss =  1.2058276297466364e-06\n",
            "batch =  3 loss =  1.8037501376966247e-06\n",
            "batch =  4 loss =  2.16467469726922e-06\n",
            "batch =  5 loss =  1.4730579778188257e-06\n",
            "batch =  6 loss =  9.173236321657896e-07\n",
            "batch =  7 loss =  1.2997878684473108e-06\n",
            "batch =  8 loss =  6.586374183825683e-07\n",
            "batch =  9 loss =  1.43616716741235e-06\n",
            "day =  119 cumReward = 0.039403009577986016 pnl = 0.023197764086092865\n",
            "batch =  0 loss =  1.7563040728418855e-06\n",
            "batch =  1 loss =  7.331215101658017e-07\n",
            "batch =  2 loss =  6.061944759494509e-07\n",
            "batch =  3 loss =  6.109114565333584e-07\n",
            "batch =  4 loss =  1.5695627553213853e-06\n",
            "batch =  5 loss =  6.909893954798463e-07\n",
            "batch =  6 loss =  1.527797735434433e-06\n",
            "batch =  7 loss =  1.1596250715228962e-06\n",
            "batch =  8 loss =  3.0405990401050076e-06\n",
            "batch =  9 loss =  1.98637212633912e-06\n",
            "day =  120 cumReward = 0.12326553306363207 pnl = 0.09534734047578974\n",
            "batch =  0 loss =  2.901485004258575e-06\n",
            "batch =  1 loss =  6.250774049476604e-07\n",
            "batch =  2 loss =  1.0727782182584633e-06\n",
            "batch =  3 loss =  2.0614670575014316e-06\n",
            "batch =  4 loss =  1.8365537926001707e-06\n",
            "batch =  5 loss =  1.2444273806977435e-06\n",
            "batch =  6 loss =  1.3535334346670425e-06\n",
            "batch =  7 loss =  1.1193551472388208e-06\n",
            "batch =  8 loss =  1.0901242148975143e-06\n",
            "batch =  9 loss =  1.1104777968284907e-06\n",
            "day =  121 cumReward = 0.0067700393462372215 pnl = -0.001955005997429593\n",
            "batch =  0 loss =  9.471646080783103e-07\n",
            "batch =  1 loss =  5.498861241903796e-07\n",
            "batch =  2 loss =  5.436300511973968e-07\n",
            "batch =  3 loss =  1.3405292520474177e-06\n",
            "batch =  4 loss =  9.699449492472922e-07\n",
            "batch =  5 loss =  1.9080794118053745e-06\n",
            "batch =  6 loss =  3.9582283761774306e-07\n",
            "batch =  7 loss =  9.845637123362394e-07\n",
            "batch =  8 loss =  8.301040566038864e-07\n",
            "batch =  9 loss =  1.8784802477966878e-06\n",
            "day =  122 cumReward = -0.05232399334036923 pnl = -0.05670373738069523\n",
            "batch =  0 loss =  1.4464548030446167e-06\n",
            "batch =  1 loss =  1.211115886690095e-06\n",
            "batch =  2 loss =  7.179817202995764e-07\n",
            "batch =  3 loss =  1.2923607073389576e-06\n",
            "batch =  4 loss =  6.651391686318675e-07\n",
            "batch =  5 loss =  1.7194124666275457e-06\n",
            "batch =  6 loss =  9.245641763300227e-07\n",
            "batch =  7 loss =  1.1029285360564245e-06\n",
            "batch =  8 loss =  2.810129217323265e-06\n",
            "batch =  9 loss =  6.262024498937535e-07\n",
            "day =  123 cumReward = -0.003035109934664168 pnl = -0.009582294293290605\n",
            "batch =  0 loss =  4.599118256010115e-06\n",
            "batch =  1 loss =  3.3462158626207383e-06\n",
            "batch =  2 loss =  1.5938818478389294e-06\n",
            "batch =  3 loss =  9.183671636492363e-07\n",
            "batch =  4 loss =  5.980064088362269e-07\n",
            "batch =  5 loss =  9.575940111972159e-07\n",
            "batch =  6 loss =  7.190460564743262e-07\n",
            "batch =  7 loss =  1.778125351847848e-06\n",
            "batch =  8 loss =  2.3935804165375885e-06\n",
            "batch =  9 loss =  9.451323990106175e-07\n",
            "day =  124 cumReward = 0.030836510644156177 pnl = 0.023107137054580562\n",
            "batch =  0 loss =  6.645493044743489e-07\n",
            "batch =  1 loss =  1.7948548247659346e-06\n",
            "batch =  2 loss =  2.7649657567963004e-06\n",
            "batch =  3 loss =  1.9982323919975897e-06\n",
            "batch =  4 loss =  1.0205686749031884e-06\n",
            "batch =  5 loss =  6.923568207639619e-07\n",
            "batch =  6 loss =  5.028961709285795e-07\n",
            "batch =  7 loss =  1.956905634870054e-06\n",
            "batch =  8 loss =  1.0618334727041656e-06\n",
            "batch =  9 loss =  1.3714070519199595e-06\n",
            "day =  125 cumReward = -0.02333795160359638 pnl = -0.023547746894677535\n",
            "batch =  0 loss =  1.32689649490203e-06\n",
            "batch =  1 loss =  6.592235877178609e-07\n",
            "batch =  2 loss =  7.640721264579042e-07\n",
            "batch =  3 loss =  5.557113809118164e-07\n",
            "batch =  4 loss =  7.648849305041949e-07\n",
            "batch =  5 loss =  4.5802227077729185e-07\n",
            "batch =  6 loss =  4.678307959693484e-06\n",
            "batch =  7 loss =  1.0205070793745108e-05\n",
            "batch =  8 loss =  8.574367029723362e-07\n",
            "batch =  9 loss =  4.5801238002241007e-07\n",
            "day =  126 cumReward = 0.04503741405587472 pnl = 0.024233475925726666\n",
            "batch =  0 loss =  2.5556184937158832e-06\n",
            "batch =  1 loss =  3.761947482416872e-07\n",
            "batch =  2 loss =  3.2544476198381744e-06\n",
            "batch =  3 loss =  1.3985741134092677e-06\n",
            "batch =  4 loss =  6.418301268240612e-07\n",
            "batch =  5 loss =  4.1129146666207816e-07\n",
            "batch =  6 loss =  3.4121223961847136e-06\n",
            "batch =  7 loss =  1.5050637784952414e-06\n",
            "batch =  8 loss =  6.05130844633095e-06\n",
            "batch =  9 loss =  1.166961283161072e-06\n",
            "day =  127 cumReward = 0.06348737470358558 pnl = 0.038520612283916233\n",
            "batch =  0 loss =  4.959119337399898e-07\n",
            "batch =  1 loss =  1.7870223700811039e-06\n",
            "batch =  2 loss =  6.484144705609651e-07\n",
            "batch =  3 loss =  1.1404828228478436e-06\n",
            "batch =  4 loss =  3.4240133572893683e-07\n",
            "batch =  5 loss =  2.3392851744574727e-06\n",
            "batch =  6 loss =  8.5520457560051e-07\n",
            "batch =  7 loss =  1.3323289067557198e-06\n",
            "batch =  8 loss =  9.202549904330226e-07\n",
            "batch =  9 loss =  2.7043867589782167e-07\n",
            "day =  128 cumReward = 0.03490566699171514 pnl = 0.01495353264124899\n",
            "batch =  0 loss =  4.4239536123313883e-07\n",
            "batch =  1 loss =  2.720659040278406e-07\n",
            "batch =  2 loss =  5.975491603749106e-07\n",
            "batch =  3 loss =  1.0016657370215398e-06\n",
            "batch =  4 loss =  6.817858775320929e-07\n",
            "batch =  5 loss =  6.071434199839132e-07\n",
            "batch =  6 loss =  5.085436214358197e-07\n",
            "batch =  7 loss =  2.0272152596589876e-06\n",
            "batch =  8 loss =  8.908302788768196e-07\n",
            "batch =  9 loss =  5.737762194257812e-07\n",
            "day =  129 cumReward = 0.08078741773963476 pnl = 0.041204745359654016\n",
            "batch =  0 loss =  5.156617817192455e-07\n",
            "batch =  1 loss =  1.3150988706911448e-06\n",
            "batch =  2 loss =  1.2431781897248584e-06\n",
            "batch =  3 loss =  5.876047453057254e-07\n",
            "batch =  4 loss =  1.440465780433442e-06\n",
            "batch =  5 loss =  5.547617547563277e-07\n",
            "batch =  6 loss =  4.2276482759007195e-07\n",
            "batch =  7 loss =  2.5965105123759713e-06\n",
            "batch =  8 loss =  2.692136376936105e-06\n",
            "batch =  9 loss =  1.5638574950571638e-06\n",
            "day =  130 cumReward = -0.0019625469026132982 pnl = -0.005296643587163197\n",
            "batch =  0 loss =  4.955826398145291e-07\n",
            "batch =  1 loss =  3.6441517750063213e-06\n",
            "batch =  2 loss =  4.870739758189302e-07\n",
            "batch =  3 loss =  8.071525599007146e-07\n",
            "batch =  4 loss =  2.41257851030241e-07\n",
            "batch =  5 loss =  2.3711122310032806e-07\n",
            "batch =  6 loss =  8.022891506698215e-07\n",
            "batch =  7 loss =  4.664918833441334e-07\n",
            "batch =  8 loss =  2.955175659735687e-07\n",
            "batch =  9 loss =  2.1172442643546674e-07\n",
            "day =  131 cumReward = -0.03183342082049855 pnl = -0.031606353399237674\n",
            "batch =  0 loss =  5.392317348196229e-07\n",
            "batch =  1 loss =  7.41148141969461e-07\n",
            "batch =  2 loss =  1.5216244264593115e-07\n",
            "batch =  3 loss =  3.574178890630719e-07\n",
            "batch =  4 loss =  9.413512316314154e-07\n",
            "batch =  5 loss =  2.4210024207604874e-07\n",
            "batch =  6 loss =  1.606508362783643e-06\n",
            "batch =  7 loss =  4.572118257328839e-07\n",
            "batch =  8 loss =  7.165485840232577e-07\n",
            "batch =  9 loss =  2.6112518298759824e-06\n",
            "day =  132 cumReward = -0.07442378821664687 pnl = -0.08041764425225728\n",
            "batch =  0 loss =  4.84911254261533e-07\n",
            "batch =  1 loss =  8.003943889889342e-07\n",
            "batch =  2 loss =  6.6439147303754e-06\n",
            "batch =  3 loss =  7.826874934835359e-07\n",
            "batch =  4 loss =  4.232725530073367e-07\n",
            "batch =  5 loss =  1.6745312905186438e-06\n",
            "batch =  6 loss =  1.757063046170515e-06\n",
            "batch =  7 loss =  1.48978358538443e-06\n",
            "batch =  8 loss =  7.962460131238913e-07\n",
            "batch =  9 loss =  1.9432034150668187e-06\n",
            "day =  133 cumReward = 0.01079341760151798 pnl = 0.0005990408283726989\n",
            "batch =  0 loss =  2.674152028703247e-07\n",
            "batch =  1 loss =  7.104744099706295e-07\n",
            "batch =  2 loss =  3.2769199265203497e-07\n",
            "batch =  3 loss =  1.8338212726121128e-07\n",
            "batch =  4 loss =  1.7569509509485215e-06\n",
            "batch =  5 loss =  4.8405095185444225e-06\n",
            "batch =  6 loss =  7.726873718638672e-07\n",
            "batch =  7 loss =  2.8793769502044597e-07\n",
            "batch =  8 loss =  6.458870984715759e-07\n",
            "batch =  9 loss =  3.0021413977010525e-07\n",
            "day =  134 cumReward = -0.01315998028067439 pnl = -0.01715177285844227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-b43c0c7e7d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;31m# adapt model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_cycle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_interface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_interface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-b15c6ae0b72f>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, env, model_interface, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstate_tp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstate_tp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'portfolio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpnl_state_tp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_interface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_sa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-2ddfe7950a52>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    190\u001b[0m                    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                    \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                    data['onehot']])\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \"\"\"\n\u001b[0;32m-> 1727\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4122\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4123\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4125\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2939\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3363\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    347\u001b[0m       \u001b[0mfirst_k_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[0;32m--> 349\u001b[0;31m           first_k_indices, [num_full_batches, batch_size])\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0mflat_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_k_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8232\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8233\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 8234\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   8235\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8236\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    351\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0minput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;31m# Perform input type inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6490\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6491\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6492\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6493\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6494\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4177\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytes_or_text_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4178\u001b[0;31m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4180\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_str\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'compat.as_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZZSxmlKb9Kn"
      },
      "source": [
        "# import gym\n",
        "# from gym import spaces\n",
        "\n",
        "# idea don't penalize strictly on holding even when the price falls upto a certain level\n",
        "# use the idea of stoploss i.e when the price fall below stoploss start penalizing heavily for subsequent drops\n",
        "# create levels:\n",
        "\n",
        "# rise :\n",
        "# All values  :  MULTIPLIER = 0.9\n",
        "\n",
        "# fall :\n",
        "# [   > 1.5 )%  :  MULTIPLIER = 1.5\n",
        "# [1.5 - 0.5)%  :  MULTIPLIER = 1.0\n",
        "# [0.5 - 0)%    :  MULTIPLIER = 0.8\n",
        "# [0 -  0.5)%   :  MULTIPLIER = 0.2\n",
        "# [0.5  - 1)%   :  MULTIPLIER = 1.0\n",
        "# [1 -  1.5)%   :  MULTIPLIER = 2.0\n",
        "# [1.5 -  2)%   :  MULTIPLIER = 3.0\n",
        "# [ >=   2 )%   :  MULTIPLIER = 4.0\n",
        "# [ may force it to sell ] ?\n",
        "\n",
        "class MarketEnv(object):\n",
        "\n",
        "    PENALTY = 1 #0.999756079\n",
        "    ON_PROFIT_HOLDING = 0.9\n",
        "\n",
        "    def __init__(self, X_price_info, X_indicators, X_bolinger, X_adx, X_onehot, X_nb, X_avg_val, X_close_val, X_high_val, X_low_val, scope, sudden_death = -1., cumulative_reward = False):\n",
        "        self.X_price_info = X_price_info\n",
        "        self.X_indicators = X_indicators\n",
        "        self.X_bolinger = X_bolinger\n",
        "        self.X_adx = X_adx\n",
        "        self.X_onehot = X_onehot\n",
        "        self.X_nb = X_nb\n",
        "        self.X_avg_val = X_avg_val\n",
        "        self.X_close_val = X_close_val\n",
        "        self.X_high_val = X_high_val\n",
        "        self.X_low_val = X_low_val\n",
        "        self.sudden_death = sudden_death\n",
        "        self.cumulative_reward = cumulative_reward\n",
        "        self.scope = scope\n",
        "        self.num_target = X_price_info.__len__()\n",
        "\n",
        "        self.actions = [\n",
        "            \"LONG\",\n",
        "            \"HOLD\",\n",
        "            \"SHORT\"\n",
        "        ]\n",
        "\n",
        "        self.ON_LOSS_HOLDING = [1.5, 1.2, 1.0, 0.8, 0.25, 0.9, 2.0, 3.0, 4.0]\n",
        "\n",
        "        # self.action_space = spaces.Discrete(len(self.actions))\n",
        "        # self.observation_space = spaces.Box(np.ones(scope * (len(input_codes) + 1)) * -1, np.ones(scope * (len(input_codes) + 1)))\n",
        "\n",
        "        # self.reset()\n",
        "        self._seed()\n",
        "\n",
        "    def _get_holding_mutiplier(self, cum_reward, diff):     \n",
        "        if diff >= 0:\n",
        "            return self.ON_PROFIT_HOLDING\n",
        "\n",
        "        return self.ON_LOSS_HOLDING[min(max(int((2.0 - cum_reward) / 0.5), 0), 8)]\n",
        "\n",
        "    def _create_hourly_price_till(self, index, timesteps, code):\n",
        "        minutes = timesteps * 60\n",
        "        end = min(len(self.X_close_val[code]), index + 1)\n",
        "        start = max((index + 1) % 60, index - minutes + 1)\n",
        "        j = 0\n",
        "        hourly_avg_price, hourly_close_price, hourly_high_price, hourly_low_price = [np.zeros(timesteps) for _ in range(4)]\n",
        "\n",
        "        for i in np.arange(start, end, 60):\n",
        "            hourly_avg_price[j] = np.average(self.X_avg_val[code][i: i + 60])\n",
        "            hourly_close_price[j] = self.X_close_val[code][i + 59]\n",
        "            hourly_high_price[j] = np.max(self.X_high_val[code][i: i + 60])\n",
        "            hourly_low_price[j] = np.min(self.X_low_val[code][i: i + 60])\n",
        "            j += 1\n",
        "        \n",
        "        return (hourly_avg_price, hourly_close_price, hourly_high_price, hourly_low_price, j)\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.state, self.reward, False, self.done, {}\n",
        "\n",
        "        vari = self.X_price_info[self.targetCode][self.currentDay][self.currentTargetIndex][1] / 100\n",
        "        self.reward = 0.\n",
        "        list_length = len(self.boughts)\n",
        "        self.budget = self.budget / (1 + vari)  # iter 0?\n",
        "        temp = 0.\n",
        "\n",
        "        for i in range(list_length):\n",
        "            old_pl = self.boughts[i]\n",
        "            self.boughts[i] = self.boughts[i] * MarketEnv.PENALTY * (1 + vari * self.position)\n",
        "            temp += self.boughts[i] - old_pl\n",
        "\n",
        "        # pnl of currentposition\n",
        "        cum_reward = sum(self.boughts) * self.position - len(self.boughts)\n",
        "        realized_pnl = 0.\n",
        "\n",
        "        if self.actions[action] == \"LONG\":\n",
        "            if self.position == -1:\n",
        "                for b in self.boughts:\n",
        "                    self.budget += -b\n",
        "                    self.reward += -(b + 1)\n",
        "                \n",
        "                realized_pnl = self.reward\n",
        "                if self.cumulative_reward:\n",
        "                    self.reward = self.reward / max(1, len(self.boughts))\n",
        "                \n",
        "                # if got bankrupt\n",
        "                if self.sudden_death * len(self.boughts) > self.reward:\n",
        "                    self.nextDay = True\n",
        "\n",
        "                self.boughts = []\n",
        "                self.position = 0\n",
        "            elif self.budget > 0:\n",
        "                self.boughts.append(1.0)\n",
        "                self.position = 1\n",
        "                self.budget -= 1.\n",
        "        \n",
        "        elif self.actions[action] == \"SHORT\":\n",
        "            if self.position == 1:\n",
        "                for b in self.boughts:\n",
        "                    self.budget += b\n",
        "                    self.reward += b - 1\n",
        "                \n",
        "                realized_pnl = self.reward\n",
        "                if self.cumulative_reward:\n",
        "                    self.reward = self.reward / max(1, len(self.boughts))\n",
        "\n",
        "                if self.sudden_death * len(self.boughts) > self.reward:\n",
        "                    self.nextDay = True\n",
        "\n",
        "                self.boughts = []\n",
        "                self.position = 0\n",
        "            elif self.budget > 0:\n",
        "                self.boughts.append(-1.0)\n",
        "                self.position = -1\n",
        "                self.budget -= 1.\n",
        "        \n",
        "        else:\n",
        "            temp = temp * self.position\n",
        "            self.reward = self._get_holding_mutiplier(cum_reward, temp) * temp\n",
        "\n",
        "        self.currentTargetIndex += 1\n",
        "\n",
        "        if self.currentTargetIndex >= self.episode_length or (self.budget < 1. and self.position == 0):\n",
        "            self.day_index += 1\n",
        "            self.nextday = True\n",
        "\n",
        "        if self.day_index >= self.num_days:\n",
        "            self.done = True\n",
        "        \n",
        "        self._defineState()\n",
        "        return self.state, self.reward, self.nextday, self.done, self.targetCode, realized_pnl\n",
        "\n",
        "        # ignoring this for the moment\n",
        "        # if self.done:\n",
        "            # for b in self.boughts:\n",
        "            #   self.reward += (b * self.position) - 1\n",
        "            # if self.cumulative_reward:\n",
        "            #   self.reward = self.reward / max(1, len(self.boughts))\n",
        "\n",
        "            # self.boughts = []\n",
        "\n",
        "    def reset(self):\n",
        "        self.targetCode = random.randint(0, self.num_target - 1)\n",
        "        self.num_days = self.X_price_info[self.targetCode].shape[0]\n",
        "        self.episode_length = self.X_price_info[self.targetCode].shape[1]\n",
        "        # ignore day {0, 1, 2}\n",
        "        self.attack_sequence = np.random.permutation([i for i in range(3, self.num_days)])\n",
        "        self.currentTargetIndex = 0\n",
        "        self.currentDay = 0\n",
        "        self.day_index = 0\n",
        "        self.position = 0\n",
        "        self.boughts = []\n",
        "        self.nextday = False\n",
        "        self.done = False\n",
        "        self.reward = 0.\n",
        "\n",
        "    def _render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "        return self.state\n",
        "\n",
        "    def _seed(self):\n",
        "        return int(random.random() * 100)\n",
        "\n",
        "    def nextDay(self):\n",
        "        self.currentTargetIndex = self.scope - 1\n",
        "        self.currentDay = self.attack_sequence[self.day_index]\n",
        "        self.nextday = False\n",
        "        self.boughts = []\n",
        "        self.reward = 0.\n",
        "        self.position = 0\n",
        "        self.budget = 8.        # automation required # meaning power to buy x shares at current price\n",
        "        self._defineState()\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def _create_hourly_data(self, code, day, index):\n",
        "        timesteps = 80\n",
        "        lookback = self.scope // 2\n",
        "        # adjusting day_ignored and index_ignored\n",
        "        end = (day + 4) * 375 + index + 30\n",
        "        hourly_avg_val, hourly_close_val, hourly_high_val, hourly_low_val, j = self._create_hourly_price_till(end, timesteps, code)\n",
        "        hourly_avg_val = hourly_avg_val[:j]\n",
        "        # if j != timesteps:\n",
        "          # print('lookback = ', str(lookback), 'length=', str(j), 'timestep=', str(timesteps), 'currentIndex =', str(index + 30), 'currentDay =', str(day + 4))\n",
        "        hourly_rsi = (RSI(hourly_close_val[:j], timeperiod=14)[-lookback:]) / 100\n",
        "        hourly_atr = NATR(hourly_high_val[:j], hourly_low_val[:j], hourly_close_val[:j], timeperiod=14)[-lookback:]\n",
        "        X_avg_price = (hourly_avg_val[-lookback:] - hourly_avg_val[-(lookback + 1) : -1]) * 10 / hourly_avg_val[-(lookback + 1) : -1]\n",
        "\n",
        "        X_avg_price = np.expand_dims(X_avg_price, axis=(0, 2))\n",
        "        hourly_rsi = np.expand_dims(hourly_rsi, axis=(0, 2))\n",
        "        hourly_atr = np.expand_dims(hourly_atr, axis=(0, 2))\n",
        "        hourly_data = np.expand_dims(np.concatenate([X_avg_price, hourly_rsi, hourly_atr], axis=2), axis=0)\n",
        "\n",
        "        return hourly_data\n",
        "\n",
        "    def _standardize(self, X_array):\n",
        "        n = len(X_array[0])\n",
        "        for i in range(n):\n",
        "            temp = X_array[:, i]\n",
        "            X_array[:, i] = (temp - np.mean(temp)) / np.std(temp)\n",
        "\n",
        "    # to prevent memory overflow create training data in an online way\n",
        "    def generate_data(self, code, day, index):\n",
        "        r_data = {}\n",
        "\n",
        "        if index < self.scope - 1:\n",
        "            print('index not in range =', str(index))\n",
        "            return None\n",
        "\n",
        "        # normalize prices\n",
        "        price_curve = np.array(X_price_info[code][day][index - self.scope + 1 : index + 1])\n",
        "        scaling = np.array(price_curve[self.scope // 2:, 1])\n",
        "        r_data['scale'] = np.expand_dims(scaling, axis=0)\n",
        "        self._standardize(price_curve)\n",
        "        price_curve = np.expand_dims(price_curve, axis=(0, 1))\n",
        "\n",
        "        # normalize nb prices\n",
        "        nb_price = np.array(X_price_info[code][day][index - (self.scope // 2) + 1 : index + 1])\n",
        "        self._standardize(nb_price)\n",
        "        nb_price = np.expand_dims(nb_price, axis=(0, 1))\n",
        "\n",
        "        indicators = X_indicators[code][day][index - self.scope + 1 : index + 1]\n",
        "        indicators = np.expand_dims(indicators, axis=(0, 1))\n",
        "        \n",
        "        bolinger = X_bolinger[code][day][index - self.scope + 1 : index + 1]\n",
        "        bolinger = np.swapaxes(bolinger, 0, 1)\n",
        "        bolinger = np.expand_dims(bolinger, axis=(0, 3))\n",
        "        \n",
        "        adx = X_adx[code][day][index - self.scope + 1 : index + 1]\n",
        "        adx = np.expand_dims(adx, axis=(0, 1))\n",
        "        \n",
        "        onehot = X_onehot[code][day][index - (self.scope // 2) + 1 : index + 1]\n",
        "        onehot = np.swapaxes(onehot, 0, 1)\n",
        "        onehot = np.expand_dims(onehot, axis=(0, 3))\n",
        "\n",
        "        r_data['price_curve'] = price_curve\n",
        "        r_data['indicators'] = indicators\n",
        "        r_data['bolinger'] = bolinger\n",
        "        r_data['adx'] = adx\n",
        "        r_data['onehot'] = onehot\n",
        "        r_data['nb'] = nb_price\n",
        "        r_data['hourly_data'] = self._create_hourly_data(code, day, index)\n",
        "\n",
        "        return r_data\n",
        "\n",
        "    def _defineState(self):\n",
        "        if self.nextday or self.done:\n",
        "            return\n",
        "\n",
        "        this_PnL = (sum(self.boughts) * self.position - len(self.boughts)) / 8\n",
        "\n",
        "        tmpState = self.generate_data(self.targetCode, self.currentDay, self.currentTargetIndex)\n",
        "        tmpState['code'] = self.targetCode\n",
        "        tmpState['day'] = self.currentDay\n",
        "        tmpState['index'] = self.currentTargetIndex\n",
        "        # markovian in nature\n",
        "        onehot_code = np.zeros(self.num_target, dtype=float)\n",
        "        onehot_code[self.targetCode] = 1.0\n",
        "        tmpState['portfolio'] = np.expand_dims(np.concatenate([np.array([this_PnL,\n",
        "                                                            self.budget / 8,\n",
        "                                                            self.position,\n",
        "                                                            float(self.currentDay) / self.episode_length]),\n",
        "                                                            onehot_code], axis=0), axis=0)\n",
        "        self.state = tmpState"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxfnNg8FJpUc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}